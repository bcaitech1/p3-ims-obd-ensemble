{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T11:06:58.944902Z",
     "start_time": "2021-04-22T11:06:56.623974Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pytorch version: 1.4.0\nGPU 사용 가능 여부: True\nTesla V100-PCIE-32GB\n1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import warnings \n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from model import *\n",
    "from utils import *\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 전처리를 위한 라이브러리\n",
    "from pycocotools.coco import COCO\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# 시각화를 위한 라이브러리\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from adamp import AdamP\n",
    "\n",
    "plt.rcParams['axes.grid'] = False\n",
    "\n",
    "print('pytorch version: {}'.format(torch.__version__))\n",
    "print('GPU 사용 가능 여부: {}'.format(torch.cuda.is_available()))\n",
    "\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"   # GPU 사용 가능 여부에 따라 device 정보 저장"
   ]
  },
  {
   "source": [
    "# EMANet"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "성공한 실험 version05으로부터 이후에 lr 5e-5실험이후 "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모든 하이퍼파라미터는 여기서 세팅 및 seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = {'FCN8s':FCN8s, 'EMANet': EMANet}\n",
    "loss_list = {'CE':nn.CrossEntropyLoss,}\n",
    "optim_list = {'Adam':optim.Adam, 'AdamP':AdamP}\n",
    "scheduler_list = {'cosine_warm':CosineAnnealingWarmRestarts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T11:06:59.171980Z",
     "start_time": "2021-04-22T11:06:59.167952Z"
    }
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    version = 'v05'\n",
    "    \n",
    "    # 하이퍼 파라미터 설정\n",
    "    batch_size = 8\n",
    "    num_epochs = 130\n",
    "    lr = 5e-6\n",
    "    SEED = 21\n",
    "    num_classes = 12\n",
    "    weight_decay = 1e-6\n",
    "    loss_weight = None\n",
    "    input_size = 512\n",
    "    \n",
    "    T_0 = 10\n",
    "    T_mult = 2\n",
    "\n",
    "    # Model, Loss, Optimizer 및 하이퍼 파라미터 선택\n",
    "    model = 'EMANet' # FCN8s, EMANet,  \n",
    "    model_params = {\n",
    "        'FCN8s':{'num_classes':num_classes},\n",
    "        'EMANet':{'n_classes':12, 'n_layers':101}\n",
    "    }\n",
    "    loss = 'CE' # CE , \n",
    "    loss_params = {\n",
    "        'CE':{'weight':loss_weight},\n",
    "    }\n",
    "    optim = 'AdamP' # Adam , AdamP\n",
    "    optim_params = {\n",
    "        'Adam':{'lr':lr,'weight_decay':weight_decay},\n",
    "        'AdamP':{'lr':lr,'weight_decay':weight_decay},\n",
    "    }\n",
    "    scheduler = 'cosine_warm'\n",
    "    scheduler_params = {\n",
    "        'cosine_warm':{'T_0':T_0, 'T_mult':T_mult}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T11:06:59.446510Z",
     "start_time": "2021-04-22T11:06:59.443508Z"
    }
   },
   "outputs": [],
   "source": [
    "# seed 고정\n",
    "every_seed(CFG.SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 데이터 EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T11:07:04.139668Z",
     "start_time": "2021-04-22T11:07:00.575728Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of super categories: 11\nNumber of categories: 11\nNumber of annotations: 21116\nNumber of images: 2617\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "dataset_path = '../input/data'\n",
    "anns_file_path = dataset_path + '/' + 'train.json'\n",
    "\n",
    "# Read annotations\n",
    "with open(anns_file_path, 'r') as f:\n",
    "    dataset = json.loads(f.read())\n",
    "\n",
    "categories = dataset['categories']\n",
    "anns = dataset['annotations']\n",
    "imgs = dataset['images']\n",
    "nr_cats = len(categories)\n",
    "nr_annotations = len(anns)\n",
    "nr_images = len(imgs)\n",
    "\n",
    "# Load categories and super categories\n",
    "cat_names = []\n",
    "super_cat_names = []\n",
    "super_cat_ids = {}\n",
    "super_cat_last_name = ''\n",
    "nr_super_cats = 0\n",
    "for cat_it in categories:\n",
    "    cat_names.append(cat_it['name'])\n",
    "    super_cat_name = cat_it['supercategory']\n",
    "    # Adding new supercat\n",
    "    if super_cat_name != super_cat_last_name:\n",
    "        super_cat_names.append(super_cat_name)\n",
    "        super_cat_ids[super_cat_name] = nr_super_cats\n",
    "        super_cat_last_name = super_cat_name\n",
    "        nr_super_cats += 1\n",
    "\n",
    "print('Number of super categories:', nr_super_cats)\n",
    "print('Number of categories:', nr_cats)\n",
    "print('Number of annotations:', nr_annotations)\n",
    "print('Number of images:', nr_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T11:07:04.394832Z",
     "start_time": "2021-04-22T11:07:04.141668Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Count annotations\n",
    "cat_histogram = np.zeros(nr_cats,dtype=int)\n",
    "for ann in anns:\n",
    "    cat_histogram[ann['category_id']] += 1\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame({'Categories': cat_names, 'Number of annotations': cat_histogram})\n",
    "df = df.sort_values('Number of annotations', 0, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T11:07:04.409808Z",
     "start_time": "2021-04-22T11:07:04.395831Z"
    }
   },
   "outputs": [],
   "source": [
    "# category labeling \n",
    "sorted_temp_df = df.sort_index()\n",
    "\n",
    "# background = 0 에 해당되는 label 추가 후 기존들을 모두 label + 1 로 설정\n",
    "sorted_df = pd.DataFrame([\"Backgroud\"], columns = [\"Categories\"])\n",
    "sorted_df = sorted_df.append(sorted_temp_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리 함수 정의 (Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T11:07:04.439837Z",
     "start_time": "2021-04-22T11:07:04.425804Z"
    }
   },
   "outputs": [],
   "source": [
    "category_names = list(sorted_df.Categories)\n",
    "\n",
    "def get_classname(classID, cats):\n",
    "    for i in range(len(cats)):\n",
    "        if cats[i]['id']==classID:\n",
    "            return cats[i]['name']\n",
    "    return \"None\"\n",
    "\n",
    "class CustomDataLoader(Dataset):\n",
    "    \"\"\"COCO format\"\"\"\n",
    "    def __init__(self, data_dir, mode = 'train', transform = None):\n",
    "        super().__init__()\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.coco = COCO(data_dir)\n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        # dataset이 index되어 list처럼 동작\n",
    "        image_id = self.coco.getImgIds(imgIds=index)\n",
    "        image_infos = self.coco.loadImgs(image_id)[0]\n",
    "        \n",
    "        # cv2 를 활용하여 image 불러오기\n",
    "        images = cv2.imread(os.path.join(dataset_path, image_infos['file_name']))\n",
    "        images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        # images /= 255.0\n",
    "        \n",
    "        if (self.mode in ('train', 'val')):\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=image_infos['id'])\n",
    "            anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "            # Load the categories in a variable\n",
    "            cat_ids = self.coco.getCatIds()\n",
    "            cats = self.coco.loadCats(cat_ids)\n",
    "\n",
    "            # masks : size가 (height x width)인 2D\n",
    "            # 각각의 pixel 값에는 \"category id + 1\" 할당\n",
    "            # Background = 0\n",
    "            masks = np.zeros((image_infos[\"height\"], image_infos[\"width\"]))\n",
    "            # Unknown = 1, General trash = 2, ... , Cigarette = 11\n",
    "            for i in range(len(anns)):\n",
    "                className = get_classname(anns[i]['category_id'], cats)\n",
    "                pixel_value = category_names.index(className)\n",
    "                masks = np.maximum(self.coco.annToMask(anns[i])*pixel_value, masks)\n",
    "            masks = masks.astype(np.float32)\n",
    "\n",
    "            # transform -> albumentations 라이브러리 활용\n",
    "            if self.transform is not None:\n",
    "                transformed = self.transform(image=images, mask=masks)\n",
    "                images = transformed[\"image\"]\n",
    "                masks = transformed[\"mask\"]\n",
    "            \n",
    "            return images, masks, image_infos\n",
    "        \n",
    "        if self.mode == 'test':\n",
    "            # transform -> albumentations 라이브러리 활용\n",
    "            if self.transform is not None:\n",
    "                transformed = self.transform(image=images)\n",
    "                images = transformed[\"image\"]\n",
    "            \n",
    "            return images, image_infos\n",
    "    \n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        # 전체 dataset의 size를 return\n",
    "        return len(self.coco.getImgIds())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 정의 및 DataLoader 할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T11:07:09.179806Z",
     "start_time": "2021-04-22T11:07:04.440804Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=3.84s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.85s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# train.json / validation.json / test.json 디렉토리 설정\n",
    "train_path = dataset_path + '/train.json'\n",
    "val_path = dataset_path + '/val.json'\n",
    "test_path = dataset_path + '/test.json'\n",
    "\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "# collate_fn needs for batch\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_transform = A.Compose([\n",
    "                            A.Resize(CFG.input_size,CFG.input_size),\n",
    "                            A.Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
    "                            ToTensorV2()\n",
    "                            ])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "                          A.Resize(CFG.input_size,CFG.input_size),\n",
    "                          A.Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
    "                          ToTensorV2()\n",
    "                          ])\n",
    "\n",
    "test_transform = A.Compose([\n",
    "                           A.Resize(CFG.input_size,CFG.input_size),\n",
    "                           A.Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
    "                           ToTensorV2()\n",
    "                           ])\n",
    "\n",
    "# create own Dataset 1 (skip)\n",
    "# validation set을 직접 나누고 싶은 경우\n",
    "# random_split 사용하여 data set을 8:2 로 분할\n",
    "# train_size = int(0.8*len(dataset))\n",
    "# val_size = int(len(dataset)-train_size)\n",
    "# dataset = CustomDataLoader(data_dir=train_path, mode='train', transform=transform)\n",
    "# train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# create own Dataset 2\n",
    "# train dataset\n",
    "train_dataset = CustomDataLoader(data_dir=train_path, mode='train', transform=train_transform)\n",
    "\n",
    "# validation dataset\n",
    "val_dataset = CustomDataLoader(data_dir=val_path, mode='val', transform=val_transform)\n",
    "\n",
    "# test dataset\n",
    "test_dataset = CustomDataLoader(data_dir=test_path, mode='test', transform=test_transform)\n",
    "\n",
    "\n",
    "# DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=CFG.batch_size,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=4,\n",
    "                                           collate_fn=collate_fn,\n",
    "                                           drop_last=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                         batch_size=CFG.batch_size,\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=4,\n",
    "                                         collate_fn=collate_fn)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=5,\n",
    "                                          num_workers=4,\n",
    "                                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T11:15:34.624277Z",
     "start_time": "2021-04-22T11:15:30.068347Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input shape :  torch.Size([1, 3, 512, 512])\n",
      "output shape :  torch.Size([1, 12, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "# 구현된 model에 임의의 input을 넣어 output이 잘 나오는지 test\n",
    "model_name = CFG.model\n",
    "model = model_list[model_name](**CFG.model_params[model_name])\n",
    "\n",
    "x = torch.randn([1, 3, CFG.input_size, CFG.input_size])\n",
    "print(\"input shape : \", x.shape)\n",
    "out = model(x).to(device)\n",
    "print(\"output shape : \", out.size())\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train, validation, test 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T11:15:38.201874Z",
     "start_time": "2021-04-22T11:15:38.187884Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(num_epochs, model, data_loader, val_loader, criterion, optimizer, saved_dir, val_every, device):\n",
    "    print('Start training..')\n",
    "    best_miou = 0\n",
    "    best_loss = 9999999\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for step, (images, masks, _) in enumerate(data_loader):\n",
    "            images = torch.stack(images)       # (batch, channel, height, width)\n",
    "            masks = torch.stack(masks).long()  # (batch, channel, height, width)\n",
    "            \n",
    "            # gpu 연산을 위해 device 할당\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "                  \n",
    "            # inference\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # loss 계산 (cross entropy loss)\n",
    "            loss = criterion(outputs, masks)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # step 주기에 따른 loss 출력\n",
    "            if (step + 1) % 25 == 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(\n",
    "                    epoch+1, num_epochs, step+1, len(train_loader), loss.item()))\n",
    "        \n",
    "        # validation 주기에 따른 loss 출력 및 best model 저장\n",
    "        if (epoch + 1) % val_every == 0:\n",
    "            avrg_loss, val_miou = validation(epoch + 1, model, val_loader, criterion, device)\n",
    "            if val_miou > best_miou:\n",
    "                print('Best performance at epoch: {}'.format(epoch + 1))\n",
    "                print('Save model in', saved_dir)\n",
    "                best_miou = val_miou\n",
    "                save_model(model, saved_dir)\n",
    "            if epoch % 10 == 0 and epoch > 50:\n",
    "                torch.save(model.state_dict(), './saved/'+str(epoch)+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T11:15:38.901226Z",
     "start_time": "2021-04-22T11:15:38.888195Z"
    }
   },
   "outputs": [],
   "source": [
    "def validation(epoch, model, data_loader, criterion, device):\n",
    "    print('Start validation #{}'.format(epoch))\n",
    "    model.eval()\n",
    "    hist = np.zeros((12,12))\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        cnt = 0\n",
    "        mIoU_list = []\n",
    "        for step, (images, masks, _) in enumerate(data_loader):\n",
    "            \n",
    "            images = torch.stack(images)       # (batch, channel, height, width)\n",
    "            masks = torch.stack(masks).long()  # (batch, channel, height, width)\n",
    "\n",
    "            images, masks = images.to(device), masks.to(device)            \n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            total_loss += loss\n",
    "            cnt += 1\n",
    "            \n",
    "            outputs = torch.argmax(outputs, dim=1).detach().cpu().numpy()\n",
    "\n",
    "            hist = add_hist(hist, masks.detach().cpu().numpy(), outputs, n_class=12)\n",
    "            \n",
    "        acc, acc_cls, mIoU, fwavacc = label_accuracy_score(hist)\n",
    "        # mIoU = label_accuracy_score(masks.detach().cpu().numpy(), outputs, n_class=12)[2]\n",
    "        # mIoU_list.append(mIoU)\n",
    "            \n",
    "        avrg_loss = total_loss / cnt\n",
    "        print('Validation #{}  Average Loss: {:.4f}, mIoU: {:.4f}'.format(epoch, avrg_loss, mIoU))\n",
    "\n",
    "    return avrg_loss, mIoU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 저장 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T11:15:41.634492Z",
     "start_time": "2021-04-22T11:15:41.627493Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 저장 함수 정의\n",
    "val_every = 1 \n",
    "\n",
    "saved_dir = './saved'\n",
    "if not os.path.isdir(saved_dir):                                                           \n",
    "    os.mkdir(saved_dir)\n",
    "    \n",
    "def save_model(model, saved_dir, file_name=f'{CFG.version}.pt'):\n",
    "    check_point = {'net': model.state_dict()}\n",
    "    output_path = os.path.join(saved_dir, file_name)\n",
    "    torch.save(model.state_dict(), output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 생성 및 Loss function, Optimizer 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T11:15:43.106368Z",
     "start_time": "2021-04-22T11:15:43.096368Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loss function 정의\n",
    "loss_name = CFG.loss\n",
    "criterion = loss_list[loss_name](**CFG.loss_params[loss_name])\n",
    "\n",
    "# Optimizer 정의\n",
    "optim_name = CFG.optim\n",
    "optimizer = optim_list[optim_name](params = model.parameters(), **CFG.optim_params[optim_name])\n",
    "\n",
    "scheduler_name = CFG.scheduler\n",
    "scheduler = scheduler_list[scheduler_name](optimizer, **CFG.scheduler_params[scheduler_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-22T11:15:43.700Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Start training..\n",
      "Epoch [1/70], Step [25/655], Loss: 2.3403\n",
      "Epoch [1/70], Step [50/655], Loss: 2.0966\n",
      "Epoch [1/70], Step [75/655], Loss: 1.8673\n",
      "Epoch [1/70], Step [100/655], Loss: 1.7111\n",
      "Epoch [1/70], Step [125/655], Loss: 1.7137\n",
      "Epoch [1/70], Step [150/655], Loss: 1.5083\n",
      "Epoch [1/70], Step [175/655], Loss: 1.3585\n",
      "Epoch [1/70], Step [200/655], Loss: 1.3969\n",
      "Epoch [1/70], Step [225/655], Loss: 1.2848\n",
      "Epoch [1/70], Step [250/655], Loss: 1.1262\n",
      "Epoch [1/70], Step [275/655], Loss: 1.1980\n",
      "Epoch [1/70], Step [300/655], Loss: 1.2024\n",
      "Epoch [1/70], Step [325/655], Loss: 1.0809\n",
      "Epoch [1/70], Step [350/655], Loss: 1.1830\n",
      "Epoch [1/70], Step [375/655], Loss: 0.9493\n",
      "Epoch [1/70], Step [400/655], Loss: 1.0331\n",
      "Epoch [1/70], Step [425/655], Loss: 1.0564\n",
      "Epoch [1/70], Step [450/655], Loss: 1.3263\n",
      "Epoch [1/70], Step [475/655], Loss: 1.3671\n",
      "Epoch [1/70], Step [500/655], Loss: 0.8213\n",
      "Epoch [1/70], Step [525/655], Loss: 0.8937\n",
      "Epoch [1/70], Step [550/655], Loss: 0.9257\n",
      "Epoch [1/70], Step [575/655], Loss: 1.1558\n",
      "Epoch [1/70], Step [600/655], Loss: 0.9871\n",
      "Epoch [1/70], Step [625/655], Loss: 0.9096\n",
      "Epoch [1/70], Step [650/655], Loss: 0.7361\n",
      "Start validation #1\n",
      "Validation #1  Average Loss: 2.5116, mIoU: 0.0239\n",
      "Best performance at epoch: 1\n",
      "Save model in ./saved\n",
      "Epoch [2/70], Step [25/655], Loss: 0.8323\n",
      "Epoch [2/70], Step [50/655], Loss: 0.8851\n",
      "Epoch [2/70], Step [75/655], Loss: 0.8940\n",
      "Epoch [2/70], Step [100/655], Loss: 0.7145\n",
      "Epoch [2/70], Step [125/655], Loss: 0.7147\n",
      "Epoch [2/70], Step [150/655], Loss: 1.1611\n",
      "Epoch [2/70], Step [175/655], Loss: 0.8923\n",
      "Epoch [2/70], Step [200/655], Loss: 0.8488\n",
      "Epoch [2/70], Step [225/655], Loss: 0.7187\n",
      "Epoch [2/70], Step [250/655], Loss: 0.8090\n",
      "Epoch [2/70], Step [275/655], Loss: 0.6277\n",
      "Epoch [2/70], Step [300/655], Loss: 0.9695\n",
      "Epoch [2/70], Step [325/655], Loss: 0.7807\n",
      "Epoch [2/70], Step [350/655], Loss: 0.8826\n",
      "Epoch [2/70], Step [375/655], Loss: 1.0080\n",
      "Epoch [2/70], Step [400/655], Loss: 0.7973\n",
      "Epoch [2/70], Step [425/655], Loss: 0.6950\n",
      "Epoch [2/70], Step [450/655], Loss: 1.0491\n",
      "Epoch [2/70], Step [475/655], Loss: 0.7436\n",
      "Epoch [2/70], Step [500/655], Loss: 0.9699\n",
      "Epoch [2/70], Step [525/655], Loss: 0.8557\n",
      "Epoch [2/70], Step [550/655], Loss: 0.7660\n",
      "Epoch [2/70], Step [575/655], Loss: 0.8118\n",
      "Epoch [2/70], Step [600/655], Loss: 0.6647\n",
      "Epoch [2/70], Step [625/655], Loss: 0.5625\n",
      "Epoch [2/70], Step [650/655], Loss: 0.7011\n",
      "Start validation #2\n",
      "Validation #2  Average Loss: 2.3026, mIoU: 0.0565\n",
      "Best performance at epoch: 2\n",
      "Save model in ./saved\n",
      "Epoch [3/70], Step [25/655], Loss: 0.7395\n",
      "Epoch [3/70], Step [50/655], Loss: 0.6735\n",
      "Epoch [3/70], Step [75/655], Loss: 1.2468\n",
      "Epoch [3/70], Step [100/655], Loss: 0.6953\n",
      "Epoch [3/70], Step [125/655], Loss: 0.5091\n",
      "Epoch [3/70], Step [150/655], Loss: 0.8739\n",
      "Epoch [3/70], Step [175/655], Loss: 0.6679\n",
      "Epoch [3/70], Step [200/655], Loss: 0.5967\n",
      "Epoch [3/70], Step [225/655], Loss: 0.9181\n",
      "Epoch [3/70], Step [250/655], Loss: 0.4775\n",
      "Epoch [3/70], Step [275/655], Loss: 0.8196\n",
      "Epoch [3/70], Step [300/655], Loss: 1.0849\n",
      "Epoch [3/70], Step [325/655], Loss: 0.5917\n",
      "Epoch [3/70], Step [350/655], Loss: 0.7596\n",
      "Epoch [3/70], Step [375/655], Loss: 0.6514\n",
      "Epoch [3/70], Step [400/655], Loss: 1.1738\n",
      "Epoch [3/70], Step [425/655], Loss: 0.6963\n",
      "Epoch [3/70], Step [450/655], Loss: 0.4699\n",
      "Epoch [3/70], Step [475/655], Loss: 0.5805\n",
      "Epoch [3/70], Step [500/655], Loss: 0.7732\n",
      "Epoch [3/70], Step [525/655], Loss: 0.7528\n",
      "Epoch [3/70], Step [550/655], Loss: 0.5686\n",
      "Epoch [3/70], Step [575/655], Loss: 0.6059\n",
      "Epoch [3/70], Step [600/655], Loss: 0.7357\n",
      "Epoch [3/70], Step [625/655], Loss: 0.4926\n",
      "Epoch [3/70], Step [650/655], Loss: 0.5224\n",
      "Start validation #3\n",
      "Validation #3  Average Loss: 2.1561, mIoU: 0.0581\n",
      "Best performance at epoch: 3\n",
      "Save model in ./saved\n",
      "Epoch [4/70], Step [25/655], Loss: 0.4717\n",
      "Epoch [4/70], Step [50/655], Loss: 0.5443\n",
      "Epoch [4/70], Step [75/655], Loss: 0.5770\n",
      "Epoch [4/70], Step [100/655], Loss: 0.4424\n",
      "Epoch [4/70], Step [125/655], Loss: 0.7373\n",
      "Epoch [4/70], Step [150/655], Loss: 0.7633\n",
      "Epoch [4/70], Step [175/655], Loss: 0.5462\n",
      "Epoch [4/70], Step [200/655], Loss: 0.5111\n",
      "Epoch [4/70], Step [225/655], Loss: 0.6160\n",
      "Epoch [4/70], Step [250/655], Loss: 0.6253\n",
      "Epoch [4/70], Step [275/655], Loss: 0.4641\n",
      "Epoch [4/70], Step [300/655], Loss: 0.7233\n",
      "Epoch [4/70], Step [325/655], Loss: 0.4313\n",
      "Epoch [4/70], Step [350/655], Loss: 0.4012\n",
      "Epoch [4/70], Step [375/655], Loss: 0.3767\n",
      "Epoch [4/70], Step [400/655], Loss: 0.5055\n",
      "Epoch [4/70], Step [425/655], Loss: 0.6194\n",
      "Epoch [4/70], Step [450/655], Loss: 0.6001\n",
      "Epoch [4/70], Step [475/655], Loss: 0.4452\n",
      "Epoch [4/70], Step [500/655], Loss: 0.4555\n",
      "Epoch [4/70], Step [525/655], Loss: 0.5594\n",
      "Epoch [4/70], Step [550/655], Loss: 0.4636\n",
      "Epoch [4/70], Step [575/655], Loss: 0.5634\n",
      "Epoch [4/70], Step [600/655], Loss: 0.4907\n",
      "Epoch [4/70], Step [625/655], Loss: 0.4727\n",
      "Epoch [4/70], Step [650/655], Loss: 0.4144\n",
      "Start validation #4\n",
      "Validation #4  Average Loss: 2.0222, mIoU: 0.0582\n",
      "Best performance at epoch: 4\n",
      "Save model in ./saved\n",
      "Epoch [5/70], Step [25/655], Loss: 0.3254\n",
      "Epoch [5/70], Step [50/655], Loss: 0.7081\n",
      "Epoch [5/70], Step [75/655], Loss: 0.6106\n",
      "Epoch [5/70], Step [100/655], Loss: 0.8149\n",
      "Epoch [5/70], Step [125/655], Loss: 0.4703\n",
      "Epoch [5/70], Step [150/655], Loss: 0.3671\n",
      "Epoch [5/70], Step [175/655], Loss: 1.0107\n",
      "Epoch [5/70], Step [200/655], Loss: 0.3406\n",
      "Epoch [5/70], Step [225/655], Loss: 0.8066\n",
      "Epoch [5/70], Step [250/655], Loss: 0.4010\n",
      "Epoch [5/70], Step [275/655], Loss: 0.3388\n",
      "Epoch [5/70], Step [300/655], Loss: 0.6959\n",
      "Epoch [5/70], Step [325/655], Loss: 0.3778\n",
      "Epoch [5/70], Step [350/655], Loss: 0.5804\n",
      "Epoch [5/70], Step [375/655], Loss: 0.4310\n",
      "Epoch [5/70], Step [400/655], Loss: 0.3070\n",
      "Epoch [5/70], Step [425/655], Loss: 0.5174\n",
      "Epoch [5/70], Step [450/655], Loss: 0.5259\n",
      "Epoch [5/70], Step [475/655], Loss: 0.7088\n",
      "Epoch [5/70], Step [500/655], Loss: 0.4708\n",
      "Epoch [5/70], Step [525/655], Loss: 0.5098\n",
      "Epoch [5/70], Step [550/655], Loss: 0.4072\n",
      "Epoch [5/70], Step [575/655], Loss: 0.5396\n",
      "Epoch [5/70], Step [600/655], Loss: 0.3727\n",
      "Epoch [5/70], Step [625/655], Loss: 0.3369\n",
      "Epoch [5/70], Step [650/655], Loss: 0.4365\n",
      "Start validation #5\n",
      "Validation #5  Average Loss: 1.8620, mIoU: 0.0582\n",
      "Best performance at epoch: 5\n",
      "Save model in ./saved\n",
      "Epoch [6/70], Step [25/655], Loss: 0.4360\n",
      "Epoch [6/70], Step [50/655], Loss: 0.4933\n",
      "Epoch [6/70], Step [75/655], Loss: 0.6215\n",
      "Epoch [6/70], Step [100/655], Loss: 0.3939\n",
      "Epoch [6/70], Step [125/655], Loss: 0.5473\n",
      "Epoch [6/70], Step [150/655], Loss: 0.2664\n",
      "Epoch [6/70], Step [175/655], Loss: 0.4780\n",
      "Epoch [6/70], Step [200/655], Loss: 0.2972\n",
      "Epoch [6/70], Step [225/655], Loss: 0.9598\n",
      "Epoch [6/70], Step [250/655], Loss: 0.3867\n",
      "Epoch [6/70], Step [275/655], Loss: 0.4538\n",
      "Epoch [6/70], Step [300/655], Loss: 0.3014\n",
      "Epoch [6/70], Step [325/655], Loss: 0.3423\n",
      "Epoch [6/70], Step [350/655], Loss: 0.4485\n",
      "Epoch [6/70], Step [375/655], Loss: 0.3640\n",
      "Epoch [6/70], Step [400/655], Loss: 0.2514\n",
      "Epoch [6/70], Step [425/655], Loss: 0.3363\n",
      "Epoch [6/70], Step [450/655], Loss: 0.4403\n",
      "Epoch [6/70], Step [475/655], Loss: 0.1975\n",
      "Epoch [6/70], Step [500/655], Loss: 0.2827\n",
      "Epoch [6/70], Step [525/655], Loss: 0.3935\n",
      "Epoch [6/70], Step [550/655], Loss: 0.2413\n",
      "Epoch [6/70], Step [575/655], Loss: 0.4016\n",
      "Epoch [6/70], Step [600/655], Loss: 0.4282\n",
      "Epoch [6/70], Step [625/655], Loss: 0.3979\n",
      "Epoch [6/70], Step [650/655], Loss: 0.3880\n",
      "Start validation #6\n",
      "Validation #6  Average Loss: 1.6764, mIoU: 0.0582\n",
      "Epoch [7/70], Step [25/655], Loss: 0.2977\n",
      "Epoch [7/70], Step [50/655], Loss: 0.3904\n",
      "Epoch [7/70], Step [75/655], Loss: 0.7783\n",
      "Epoch [7/70], Step [100/655], Loss: 0.2361\n",
      "Epoch [7/70], Step [125/655], Loss: 0.3908\n",
      "Epoch [7/70], Step [150/655], Loss: 0.4380\n",
      "Epoch [7/70], Step [175/655], Loss: 0.3505\n",
      "Epoch [7/70], Step [200/655], Loss: 0.3957\n",
      "Epoch [7/70], Step [225/655], Loss: 0.2328\n",
      "Epoch [7/70], Step [250/655], Loss: 0.2869\n",
      "Epoch [7/70], Step [275/655], Loss: 0.2316\n",
      "Epoch [7/70], Step [300/655], Loss: 0.2609\n",
      "Epoch [7/70], Step [325/655], Loss: 0.3324\n",
      "Epoch [7/70], Step [350/655], Loss: 0.2699\n",
      "Epoch [7/70], Step [375/655], Loss: 0.1905\n",
      "Epoch [7/70], Step [400/655], Loss: 0.4044\n",
      "Epoch [7/70], Step [425/655], Loss: 0.2671\n",
      "Epoch [7/70], Step [450/655], Loss: 0.3181\n",
      "Epoch [7/70], Step [475/655], Loss: 0.2593\n",
      "Epoch [7/70], Step [500/655], Loss: 0.1536\n",
      "Epoch [7/70], Step [525/655], Loss: 0.2300\n",
      "Epoch [7/70], Step [550/655], Loss: 0.2852\n",
      "Epoch [7/70], Step [575/655], Loss: 0.4530\n",
      "Epoch [7/70], Step [600/655], Loss: 0.1791\n",
      "Epoch [7/70], Step [625/655], Loss: 0.2122\n",
      "Epoch [7/70], Step [650/655], Loss: 0.2288\n",
      "Start validation #7\n",
      "Validation #7  Average Loss: 1.5602, mIoU: 0.0582\n",
      "Epoch [8/70], Step [25/655], Loss: 0.3071\n",
      "Epoch [8/70], Step [50/655], Loss: 0.2373\n",
      "Epoch [8/70], Step [75/655], Loss: 0.4603\n",
      "Epoch [8/70], Step [100/655], Loss: 0.6229\n",
      "Epoch [8/70], Step [125/655], Loss: 0.2941\n",
      "Epoch [8/70], Step [150/655], Loss: 0.2547\n",
      "Epoch [8/70], Step [175/655], Loss: 0.2076\n",
      "Epoch [8/70], Step [200/655], Loss: 0.2625\n",
      "Epoch [8/70], Step [225/655], Loss: 0.4301\n",
      "Epoch [8/70], Step [250/655], Loss: 0.2680\n",
      "Epoch [8/70], Step [275/655], Loss: 0.2751\n",
      "Epoch [8/70], Step [300/655], Loss: 0.1703\n",
      "Epoch [8/70], Step [325/655], Loss: 0.2662\n",
      "Epoch [8/70], Step [350/655], Loss: 0.2218\n",
      "Epoch [8/70], Step [375/655], Loss: 0.5751\n",
      "Epoch [8/70], Step [400/655], Loss: 0.2656\n",
      "Epoch [8/70], Step [425/655], Loss: 0.3108\n",
      "Epoch [8/70], Step [450/655], Loss: 0.1510\n",
      "Epoch [8/70], Step [475/655], Loss: 0.1756\n",
      "Epoch [8/70], Step [500/655], Loss: 0.2196\n",
      "Epoch [8/70], Step [525/655], Loss: 0.2752\n",
      "Epoch [8/70], Step [550/655], Loss: 0.3139\n",
      "Epoch [8/70], Step [575/655], Loss: 0.2562\n",
      "Epoch [8/70], Step [600/655], Loss: 0.2221\n",
      "Epoch [8/70], Step [625/655], Loss: 0.2488\n",
      "Epoch [8/70], Step [650/655], Loss: 0.2001\n",
      "Start validation #8\n",
      "Validation #8  Average Loss: 1.4806, mIoU: 0.0582\n",
      "Best performance at epoch: 8\n",
      "Save model in ./saved\n",
      "Epoch [9/70], Step [25/655], Loss: 0.2773\n",
      "Epoch [9/70], Step [50/655], Loss: 0.1549\n",
      "Epoch [9/70], Step [75/655], Loss: 0.2180\n",
      "Epoch [9/70], Step [100/655], Loss: 0.2930\n",
      "Epoch [9/70], Step [125/655], Loss: 0.2916\n",
      "Epoch [9/70], Step [150/655], Loss: 0.2284\n",
      "Epoch [9/70], Step [175/655], Loss: 0.4503\n",
      "Epoch [9/70], Step [200/655], Loss: 0.2377\n",
      "Epoch [9/70], Step [225/655], Loss: 0.1952\n",
      "Epoch [9/70], Step [250/655], Loss: 0.3171\n",
      "Epoch [9/70], Step [275/655], Loss: 0.2366\n",
      "Epoch [9/70], Step [300/655], Loss: 0.1851\n",
      "Epoch [9/70], Step [325/655], Loss: 0.1915\n",
      "Epoch [9/70], Step [350/655], Loss: 0.2031\n",
      "Epoch [9/70], Step [375/655], Loss: 0.1342\n",
      "Epoch [9/70], Step [400/655], Loss: 0.2692\n",
      "Epoch [9/70], Step [425/655], Loss: 0.3563\n",
      "Epoch [9/70], Step [450/655], Loss: 0.1840\n",
      "Epoch [9/70], Step [475/655], Loss: 0.1656\n",
      "Epoch [9/70], Step [500/655], Loss: 0.1272\n",
      "Epoch [9/70], Step [525/655], Loss: 0.3151\n",
      "Epoch [9/70], Step [550/655], Loss: 0.2889\n",
      "Epoch [9/70], Step [575/655], Loss: 0.2227\n",
      "Epoch [9/70], Step [600/655], Loss: 0.2433\n",
      "Epoch [9/70], Step [625/655], Loss: 0.1652\n",
      "Epoch [9/70], Step [650/655], Loss: 0.1625\n",
      "Start validation #9\n",
      "Validation #9  Average Loss: 1.3625, mIoU: 0.0582\n",
      "Epoch [10/70], Step [25/655], Loss: 0.1937\n",
      "Epoch [10/70], Step [50/655], Loss: 0.3115\n",
      "Epoch [10/70], Step [75/655], Loss: 0.1366\n",
      "Epoch [10/70], Step [100/655], Loss: 0.1420\n",
      "Epoch [10/70], Step [125/655], Loss: 0.2104\n",
      "Epoch [10/70], Step [150/655], Loss: 0.4189\n",
      "Epoch [10/70], Step [175/655], Loss: 0.2266\n",
      "Epoch [10/70], Step [200/655], Loss: 0.1537\n",
      "Epoch [10/70], Step [225/655], Loss: 0.2145\n",
      "Epoch [10/70], Step [250/655], Loss: 0.1848\n",
      "Epoch [10/70], Step [275/655], Loss: 0.2031\n",
      "Epoch [10/70], Step [300/655], Loss: 0.2126\n",
      "Epoch [10/70], Step [325/655], Loss: 0.1603\n",
      "Epoch [10/70], Step [350/655], Loss: 0.2720\n",
      "Epoch [10/70], Step [375/655], Loss: 0.2297\n",
      "Epoch [10/70], Step [400/655], Loss: 0.2148\n",
      "Epoch [10/70], Step [425/655], Loss: 0.2059\n",
      "Epoch [10/70], Step [450/655], Loss: 0.2141\n",
      "Epoch [10/70], Step [475/655], Loss: 0.1472\n",
      "Epoch [10/70], Step [500/655], Loss: 0.1713\n",
      "Epoch [10/70], Step [525/655], Loss: 0.3588\n",
      "Epoch [10/70], Step [550/655], Loss: 0.2665\n",
      "Epoch [10/70], Step [575/655], Loss: 0.1583\n",
      "Epoch [10/70], Step [600/655], Loss: 0.1588\n",
      "Epoch [10/70], Step [625/655], Loss: 0.1242\n",
      "Epoch [10/70], Step [650/655], Loss: 0.3942\n",
      "Start validation #10\n",
      "Validation #10  Average Loss: 1.3079, mIoU: 0.0582\n",
      "Epoch [11/70], Step [25/655], Loss: 0.1846\n",
      "Epoch [11/70], Step [50/655], Loss: 0.1269\n",
      "Epoch [11/70], Step [75/655], Loss: 0.1421\n",
      "Epoch [11/70], Step [100/655], Loss: 0.1579\n",
      "Epoch [11/70], Step [125/655], Loss: 0.1431\n",
      "Epoch [11/70], Step [150/655], Loss: 0.1603\n",
      "Epoch [11/70], Step [175/655], Loss: 0.2368\n",
      "Epoch [11/70], Step [200/655], Loss: 0.2344\n",
      "Epoch [11/70], Step [225/655], Loss: 0.2516\n",
      "Epoch [11/70], Step [250/655], Loss: 0.2077\n",
      "Epoch [11/70], Step [275/655], Loss: 0.1858\n",
      "Epoch [11/70], Step [300/655], Loss: 0.1306\n",
      "Epoch [11/70], Step [325/655], Loss: 0.2615\n",
      "Epoch [11/70], Step [350/655], Loss: 0.2044\n",
      "Epoch [11/70], Step [375/655], Loss: 0.1584\n",
      "Epoch [11/70], Step [400/655], Loss: 0.1346\n",
      "Epoch [11/70], Step [425/655], Loss: 0.3367\n",
      "Epoch [11/70], Step [450/655], Loss: 0.2364\n",
      "Epoch [11/70], Step [475/655], Loss: 0.1514\n",
      "Epoch [11/70], Step [500/655], Loss: 0.3014\n",
      "Epoch [11/70], Step [525/655], Loss: 0.2164\n",
      "Epoch [11/70], Step [550/655], Loss: 0.2082\n",
      "Epoch [11/70], Step [575/655], Loss: 0.2033\n",
      "Epoch [11/70], Step [600/655], Loss: 0.2838\n",
      "Epoch [11/70], Step [625/655], Loss: 0.2089\n",
      "Epoch [11/70], Step [650/655], Loss: 0.1855\n",
      "Start validation #11\n",
      "Validation #11  Average Loss: 1.2542, mIoU: 0.0582\n",
      "Best performance at epoch: 11\n",
      "Save model in ./saved\n",
      "Epoch [12/70], Step [25/655], Loss: 0.1403\n",
      "Epoch [12/70], Step [50/655], Loss: 0.4183\n",
      "Epoch [12/70], Step [75/655], Loss: 0.1287\n",
      "Epoch [12/70], Step [100/655], Loss: 0.1231\n",
      "Epoch [12/70], Step [125/655], Loss: 0.1434\n",
      "Epoch [12/70], Step [150/655], Loss: 0.3411\n",
      "Epoch [12/70], Step [175/655], Loss: 0.1377\n",
      "Epoch [12/70], Step [200/655], Loss: 0.1624\n",
      "Epoch [12/70], Step [225/655], Loss: 0.1704\n",
      "Epoch [12/70], Step [250/655], Loss: 0.2553\n",
      "Epoch [12/70], Step [275/655], Loss: 0.1680\n",
      "Epoch [12/70], Step [300/655], Loss: 0.1291\n",
      "Epoch [12/70], Step [325/655], Loss: 0.1233\n",
      "Epoch [12/70], Step [350/655], Loss: 0.0931\n",
      "Epoch [12/70], Step [375/655], Loss: 0.1296\n",
      "Epoch [12/70], Step [400/655], Loss: 0.1126\n",
      "Epoch [12/70], Step [425/655], Loss: 0.1483\n",
      "Epoch [12/70], Step [450/655], Loss: 0.1740\n",
      "Epoch [12/70], Step [475/655], Loss: 0.2097\n",
      "Epoch [12/70], Step [500/655], Loss: 0.1178\n",
      "Epoch [12/70], Step [525/655], Loss: 0.1169\n",
      "Epoch [12/70], Step [550/655], Loss: 0.1401\n",
      "Epoch [12/70], Step [575/655], Loss: 0.3670\n",
      "Epoch [12/70], Step [600/655], Loss: 0.3725\n",
      "Epoch [12/70], Step [625/655], Loss: 0.1827\n",
      "Epoch [12/70], Step [650/655], Loss: 0.1024\n",
      "Start validation #12\n",
      "Validation #12  Average Loss: 1.2057, mIoU: 0.0584\n",
      "Best performance at epoch: 12\n",
      "Save model in ./saved\n",
      "Epoch [13/70], Step [25/655], Loss: 0.2089\n",
      "Epoch [13/70], Step [50/655], Loss: 0.1484\n",
      "Epoch [13/70], Step [75/655], Loss: 0.1278\n",
      "Epoch [13/70], Step [100/655], Loss: 0.0619\n",
      "Epoch [13/70], Step [125/655], Loss: 0.1874\n",
      "Epoch [13/70], Step [150/655], Loss: 0.0963\n",
      "Epoch [13/70], Step [175/655], Loss: 0.0971\n",
      "Epoch [13/70], Step [200/655], Loss: 0.2560\n",
      "Epoch [13/70], Step [225/655], Loss: 0.2467\n",
      "Epoch [13/70], Step [250/655], Loss: 0.1157\n",
      "Epoch [13/70], Step [275/655], Loss: 0.2337\n",
      "Epoch [13/70], Step [300/655], Loss: 0.2392\n",
      "Epoch [13/70], Step [325/655], Loss: 0.2596\n",
      "Epoch [13/70], Step [350/655], Loss: 0.1554\n",
      "Epoch [13/70], Step [375/655], Loss: 0.3504\n",
      "Epoch [13/70], Step [400/655], Loss: 0.1395\n",
      "Epoch [13/70], Step [425/655], Loss: 0.1400\n",
      "Epoch [13/70], Step [450/655], Loss: 0.1286\n",
      "Epoch [13/70], Step [475/655], Loss: 0.1680\n",
      "Epoch [13/70], Step [500/655], Loss: 0.0990\n",
      "Epoch [13/70], Step [525/655], Loss: 0.1090\n",
      "Epoch [13/70], Step [550/655], Loss: 0.1296\n",
      "Epoch [13/70], Step [575/655], Loss: 0.1397\n",
      "Epoch [13/70], Step [600/655], Loss: 0.0889\n",
      "Epoch [13/70], Step [625/655], Loss: 0.1356\n",
      "Epoch [13/70], Step [650/655], Loss: 0.1087\n",
      "Start validation #13\n",
      "Validation #13  Average Loss: 1.2500, mIoU: 0.0583\n",
      "Epoch [14/70], Step [25/655], Loss: 0.1003\n",
      "Epoch [14/70], Step [50/655], Loss: 0.0749\n",
      "Epoch [14/70], Step [75/655], Loss: 0.1981\n",
      "Epoch [14/70], Step [100/655], Loss: 0.1901\n",
      "Epoch [14/70], Step [125/655], Loss: 0.0980\n",
      "Epoch [14/70], Step [150/655], Loss: 0.1625\n",
      "Epoch [14/70], Step [175/655], Loss: 0.2426\n",
      "Epoch [14/70], Step [200/655], Loss: 0.0810\n",
      "Epoch [14/70], Step [225/655], Loss: 0.1497\n",
      "Epoch [14/70], Step [250/655], Loss: 0.1592\n",
      "Epoch [14/70], Step [275/655], Loss: 0.1071\n",
      "Epoch [14/70], Step [300/655], Loss: 0.1607\n",
      "Epoch [14/70], Step [325/655], Loss: 0.1443\n",
      "Epoch [14/70], Step [350/655], Loss: 0.1153\n",
      "Epoch [14/70], Step [375/655], Loss: 0.0934\n",
      "Epoch [14/70], Step [400/655], Loss: 0.1519\n",
      "Epoch [14/70], Step [425/655], Loss: 0.1278\n",
      "Epoch [14/70], Step [450/655], Loss: 0.1608\n",
      "Epoch [14/70], Step [475/655], Loss: 0.1101\n",
      "Epoch [14/70], Step [500/655], Loss: 0.1582\n",
      "Epoch [14/70], Step [525/655], Loss: 0.0754\n",
      "Epoch [14/70], Step [550/655], Loss: 0.2326\n",
      "Epoch [14/70], Step [575/655], Loss: 0.1152\n",
      "Epoch [14/70], Step [600/655], Loss: 0.0791\n",
      "Epoch [14/70], Step [625/655], Loss: 0.0901\n",
      "Epoch [14/70], Step [650/655], Loss: 0.0744\n",
      "Start validation #14\n",
      "Validation #14  Average Loss: 0.9919, mIoU: 0.0798\n",
      "Best performance at epoch: 14\n",
      "Save model in ./saved\n",
      "Epoch [15/70], Step [25/655], Loss: 0.1349\n",
      "Epoch [15/70], Step [50/655], Loss: 0.1796\n",
      "Epoch [15/70], Step [75/655], Loss: 0.1862\n",
      "Epoch [15/70], Step [100/655], Loss: 0.1543\n",
      "Epoch [15/70], Step [125/655], Loss: 0.1513\n",
      "Epoch [15/70], Step [150/655], Loss: 0.1306\n",
      "Epoch [15/70], Step [175/655], Loss: 0.0861\n",
      "Epoch [15/70], Step [200/655], Loss: 0.1176\n",
      "Epoch [15/70], Step [225/655], Loss: 0.1035\n",
      "Epoch [15/70], Step [250/655], Loss: 0.1189\n",
      "Epoch [15/70], Step [275/655], Loss: 0.0739\n",
      "Epoch [15/70], Step [300/655], Loss: 0.1177\n",
      "Epoch [15/70], Step [325/655], Loss: 0.0960\n",
      "Epoch [15/70], Step [350/655], Loss: 0.2126\n",
      "Epoch [15/70], Step [375/655], Loss: 0.1052\n",
      "Epoch [15/70], Step [400/655], Loss: 0.2118\n",
      "Epoch [15/70], Step [425/655], Loss: 0.1433\n",
      "Epoch [15/70], Step [450/655], Loss: 0.1187\n",
      "Epoch [15/70], Step [475/655], Loss: 0.0854\n",
      "Epoch [15/70], Step [500/655], Loss: 0.0660\n",
      "Epoch [15/70], Step [525/655], Loss: 0.1084\n",
      "Epoch [15/70], Step [550/655], Loss: 0.1892\n",
      "Epoch [15/70], Step [575/655], Loss: 0.1258\n",
      "Epoch [15/70], Step [600/655], Loss: 0.1056\n",
      "Epoch [15/70], Step [625/655], Loss: 0.0956\n",
      "Epoch [15/70], Step [650/655], Loss: 0.1489\n",
      "Start validation #15\n",
      "Validation #15  Average Loss: 1.1922, mIoU: 0.0663\n",
      "Epoch [16/70], Step [25/655], Loss: 0.1257\n",
      "Epoch [16/70], Step [50/655], Loss: 0.2230\n",
      "Epoch [16/70], Step [75/655], Loss: 0.1024\n",
      "Epoch [16/70], Step [100/655], Loss: 0.3511\n",
      "Epoch [16/70], Step [125/655], Loss: 0.1434\n",
      "Epoch [16/70], Step [150/655], Loss: 0.2467\n",
      "Epoch [16/70], Step [175/655], Loss: 0.0866\n",
      "Epoch [16/70], Step [200/655], Loss: 0.1984\n",
      "Epoch [16/70], Step [225/655], Loss: 0.0810\n",
      "Epoch [16/70], Step [250/655], Loss: 0.0979\n",
      "Epoch [16/70], Step [275/655], Loss: 0.1510\n",
      "Epoch [16/70], Step [300/655], Loss: 0.1287\n",
      "Epoch [16/70], Step [325/655], Loss: 0.1326\n",
      "Epoch [16/70], Step [350/655], Loss: 0.2055\n",
      "Epoch [16/70], Step [375/655], Loss: 0.0751\n",
      "Epoch [16/70], Step [400/655], Loss: 0.1715\n",
      "Epoch [16/70], Step [425/655], Loss: 0.0652\n",
      "Epoch [16/70], Step [450/655], Loss: 0.1597\n",
      "Epoch [16/70], Step [475/655], Loss: 0.0721\n",
      "Epoch [16/70], Step [500/655], Loss: 0.1457\n",
      "Epoch [16/70], Step [525/655], Loss: 0.0509\n",
      "Epoch [16/70], Step [550/655], Loss: 0.0783\n",
      "Epoch [16/70], Step [575/655], Loss: 0.1597\n",
      "Epoch [16/70], Step [600/655], Loss: 0.1082\n",
      "Epoch [16/70], Step [625/655], Loss: 0.0624\n",
      "Epoch [16/70], Step [650/655], Loss: 0.1180\n",
      "Start validation #16\n",
      "Validation #16  Average Loss: 1.3621, mIoU: 0.0586\n",
      "Epoch [17/70], Step [25/655], Loss: 0.1182\n",
      "Epoch [17/70], Step [50/655], Loss: 0.1488\n",
      "Epoch [17/70], Step [75/655], Loss: 0.1264\n",
      "Epoch [17/70], Step [100/655], Loss: 0.1146\n",
      "Epoch [17/70], Step [125/655], Loss: 0.0728\n",
      "Epoch [17/70], Step [150/655], Loss: 0.1304\n",
      "Epoch [17/70], Step [175/655], Loss: 0.0963\n",
      "Epoch [17/70], Step [200/655], Loss: 0.1027\n",
      "Epoch [17/70], Step [225/655], Loss: 0.0862\n",
      "Epoch [17/70], Step [250/655], Loss: 0.0693\n",
      "Epoch [17/70], Step [275/655], Loss: 0.1080\n",
      "Epoch [17/70], Step [300/655], Loss: 0.1929\n",
      "Epoch [17/70], Step [325/655], Loss: 0.0549\n",
      "Epoch [17/70], Step [350/655], Loss: 0.0717\n",
      "Epoch [17/70], Step [375/655], Loss: 0.1131\n",
      "Epoch [17/70], Step [400/655], Loss: 0.1587\n",
      "Epoch [17/70], Step [425/655], Loss: 0.0926\n",
      "Epoch [17/70], Step [450/655], Loss: 0.1529\n",
      "Epoch [17/70], Step [475/655], Loss: 0.0776\n",
      "Epoch [17/70], Step [500/655], Loss: 0.0643\n",
      "Epoch [17/70], Step [525/655], Loss: 0.1542\n",
      "Epoch [17/70], Step [550/655], Loss: 0.0856\n",
      "Epoch [17/70], Step [575/655], Loss: 0.1152\n",
      "Epoch [17/70], Step [600/655], Loss: 0.0685\n",
      "Epoch [17/70], Step [625/655], Loss: 0.0646\n",
      "Epoch [17/70], Step [650/655], Loss: 0.1085\n",
      "Start validation #17\n",
      "Validation #17  Average Loss: 1.2710, mIoU: 0.0612\n",
      "Epoch [18/70], Step [25/655], Loss: 0.0671\n",
      "Epoch [18/70], Step [50/655], Loss: 0.0869\n",
      "Epoch [18/70], Step [75/655], Loss: 0.0818\n",
      "Epoch [18/70], Step [100/655], Loss: 0.2009\n",
      "Epoch [18/70], Step [125/655], Loss: 0.0470\n",
      "Epoch [18/70], Step [150/655], Loss: 0.1424\n",
      "Epoch [18/70], Step [175/655], Loss: 0.0750\n",
      "Epoch [18/70], Step [200/655], Loss: 0.1797\n",
      "Epoch [18/70], Step [225/655], Loss: 0.0484\n",
      "Epoch [18/70], Step [250/655], Loss: 0.0975\n",
      "Epoch [18/70], Step [275/655], Loss: 0.0838\n",
      "Epoch [18/70], Step [300/655], Loss: 0.1126\n",
      "Epoch [18/70], Step [325/655], Loss: 0.2289\n",
      "Epoch [18/70], Step [350/655], Loss: 0.1209\n",
      "Epoch [18/70], Step [375/655], Loss: 0.1147\n",
      "Epoch [18/70], Step [400/655], Loss: 0.1392\n",
      "Epoch [18/70], Step [425/655], Loss: 0.0759\n",
      "Epoch [18/70], Step [450/655], Loss: 0.0876\n",
      "Epoch [18/70], Step [475/655], Loss: 0.0866\n",
      "Epoch [18/70], Step [500/655], Loss: 0.0565\n",
      "Epoch [18/70], Step [525/655], Loss: 0.0621\n",
      "Epoch [18/70], Step [550/655], Loss: 0.1158\n",
      "Epoch [18/70], Step [575/655], Loss: 0.0816\n",
      "Epoch [18/70], Step [600/655], Loss: 0.1183\n",
      "Epoch [18/70], Step [625/655], Loss: 0.0793\n",
      "Epoch [18/70], Step [650/655], Loss: 0.1192\n",
      "Start validation #18\n",
      "Validation #18  Average Loss: 1.4695, mIoU: 0.0584\n",
      "Epoch [19/70], Step [25/655], Loss: 0.0912\n",
      "Epoch [19/70], Step [50/655], Loss: 0.1516\n",
      "Epoch [19/70], Step [75/655], Loss: 0.0779\n",
      "Epoch [19/70], Step [100/655], Loss: 0.2991\n",
      "Epoch [19/70], Step [125/655], Loss: 0.0606\n",
      "Epoch [19/70], Step [150/655], Loss: 0.0988\n",
      "Epoch [19/70], Step [175/655], Loss: 0.0461\n",
      "Epoch [19/70], Step [200/655], Loss: 0.1134\n",
      "Epoch [19/70], Step [225/655], Loss: 0.0687\n",
      "Epoch [19/70], Step [250/655], Loss: 0.1500\n",
      "Epoch [19/70], Step [275/655], Loss: 0.1729\n",
      "Epoch [19/70], Step [300/655], Loss: 0.1076\n",
      "Epoch [19/70], Step [325/655], Loss: 0.0710\n",
      "Epoch [19/70], Step [350/655], Loss: 0.0803\n",
      "Epoch [19/70], Step [375/655], Loss: 0.1167\n",
      "Epoch [19/70], Step [400/655], Loss: 0.0647\n",
      "Epoch [19/70], Step [425/655], Loss: 0.1314\n",
      "Epoch [19/70], Step [450/655], Loss: 0.0890\n",
      "Epoch [19/70], Step [475/655], Loss: 0.0466\n",
      "Epoch [19/70], Step [500/655], Loss: 0.0949\n",
      "Epoch [19/70], Step [525/655], Loss: 0.1175\n",
      "Epoch [19/70], Step [550/655], Loss: 0.0755\n",
      "Epoch [19/70], Step [575/655], Loss: 0.1262\n",
      "Epoch [19/70], Step [600/655], Loss: 0.0699\n",
      "Epoch [19/70], Step [625/655], Loss: 0.0930\n",
      "Epoch [19/70], Step [650/655], Loss: 0.0550\n",
      "Start validation #19\n",
      "Validation #19  Average Loss: 1.5053, mIoU: 0.0597\n",
      "Epoch [20/70], Step [25/655], Loss: 0.1184\n",
      "Epoch [20/70], Step [50/655], Loss: 0.0904\n",
      "Epoch [20/70], Step [75/655], Loss: 0.0631\n",
      "Epoch [20/70], Step [100/655], Loss: 0.1228\n",
      "Epoch [20/70], Step [125/655], Loss: 0.1012\n",
      "Epoch [20/70], Step [150/655], Loss: 0.0505\n",
      "Epoch [20/70], Step [175/655], Loss: 0.0564\n",
      "Epoch [20/70], Step [200/655], Loss: 0.0748\n",
      "Epoch [20/70], Step [225/655], Loss: 0.0668\n",
      "Epoch [20/70], Step [250/655], Loss: 0.1682\n",
      "Epoch [20/70], Step [275/655], Loss: 0.0684\n",
      "Epoch [20/70], Step [300/655], Loss: 0.1790\n",
      "Epoch [20/70], Step [325/655], Loss: 0.1478\n",
      "Epoch [20/70], Step [350/655], Loss: 0.1606\n",
      "Epoch [20/70], Step [375/655], Loss: 0.1710\n",
      "Epoch [20/70], Step [400/655], Loss: 0.0430\n",
      "Epoch [20/70], Step [425/655], Loss: 0.1131\n",
      "Epoch [20/70], Step [450/655], Loss: 0.0885\n",
      "Epoch [20/70], Step [475/655], Loss: 0.0538\n",
      "Epoch [20/70], Step [500/655], Loss: 0.1331\n",
      "Epoch [20/70], Step [525/655], Loss: 0.3131\n",
      "Epoch [20/70], Step [550/655], Loss: 0.1535\n",
      "Epoch [20/70], Step [575/655], Loss: 0.0888\n",
      "Epoch [20/70], Step [600/655], Loss: 0.0750\n",
      "Epoch [20/70], Step [625/655], Loss: 0.1679\n",
      "Epoch [20/70], Step [650/655], Loss: 0.1154\n",
      "Start validation #20\n",
      "Validation #20  Average Loss: 0.8464, mIoU: 0.1419\n",
      "Best performance at epoch: 20\n",
      "Save model in ./saved\n",
      "Epoch [21/70], Step [25/655], Loss: 0.0807\n",
      "Epoch [21/70], Step [50/655], Loss: 0.3008\n",
      "Epoch [21/70], Step [75/655], Loss: 0.0381\n",
      "Epoch [21/70], Step [100/655], Loss: 0.0665\n",
      "Epoch [21/70], Step [125/655], Loss: 0.2297\n",
      "Epoch [21/70], Step [150/655], Loss: 0.1506\n",
      "Epoch [21/70], Step [175/655], Loss: 0.1474\n",
      "Epoch [21/70], Step [200/655], Loss: 0.0653\n",
      "Epoch [21/70], Step [225/655], Loss: 0.0924\n",
      "Epoch [21/70], Step [250/655], Loss: 0.0537\n",
      "Epoch [21/70], Step [275/655], Loss: 0.1037\n",
      "Epoch [21/70], Step [300/655], Loss: 0.1337\n",
      "Epoch [21/70], Step [325/655], Loss: 0.0479\n",
      "Epoch [21/70], Step [350/655], Loss: 0.1046\n",
      "Epoch [21/70], Step [375/655], Loss: 0.0660\n",
      "Epoch [21/70], Step [400/655], Loss: 0.2000\n",
      "Epoch [21/70], Step [425/655], Loss: 0.0459\n",
      "Epoch [21/70], Step [450/655], Loss: 0.0651\n",
      "Epoch [21/70], Step [475/655], Loss: 0.0914\n",
      "Epoch [21/70], Step [500/655], Loss: 0.0670\n",
      "Epoch [21/70], Step [525/655], Loss: 0.0799\n",
      "Epoch [21/70], Step [550/655], Loss: 0.0451\n",
      "Epoch [21/70], Step [575/655], Loss: 0.0770\n",
      "Epoch [21/70], Step [600/655], Loss: 0.1018\n",
      "Epoch [21/70], Step [625/655], Loss: 0.0731\n",
      "Epoch [21/70], Step [650/655], Loss: 0.0665\n",
      "Start validation #21\n",
      "Validation #21  Average Loss: 1.4465, mIoU: 0.0658\n",
      "Epoch [22/70], Step [25/655], Loss: 0.0532\n",
      "Epoch [22/70], Step [50/655], Loss: 0.0567\n",
      "Epoch [22/70], Step [75/655], Loss: 0.0595\n",
      "Epoch [22/70], Step [100/655], Loss: 0.0693\n",
      "Epoch [22/70], Step [125/655], Loss: 0.0672\n",
      "Epoch [22/70], Step [150/655], Loss: 0.0405\n",
      "Epoch [22/70], Step [175/655], Loss: 0.1456\n",
      "Epoch [22/70], Step [200/655], Loss: 0.0712\n",
      "Epoch [22/70], Step [225/655], Loss: 0.0585\n",
      "Epoch [22/70], Step [250/655], Loss: 0.0840\n",
      "Epoch [22/70], Step [275/655], Loss: 0.1273\n",
      "Epoch [22/70], Step [300/655], Loss: 0.0710\n",
      "Epoch [22/70], Step [325/655], Loss: 0.0514\n",
      "Epoch [22/70], Step [350/655], Loss: 0.1113\n",
      "Epoch [22/70], Step [375/655], Loss: 0.1506\n",
      "Epoch [22/70], Step [400/655], Loss: 0.1223\n",
      "Epoch [22/70], Step [425/655], Loss: 0.0885\n",
      "Epoch [22/70], Step [450/655], Loss: 0.0869\n",
      "Epoch [22/70], Step [475/655], Loss: 0.0952\n",
      "Epoch [22/70], Step [500/655], Loss: 0.0899\n",
      "Epoch [22/70], Step [525/655], Loss: 0.1468\n",
      "Epoch [22/70], Step [550/655], Loss: 0.0472\n",
      "Epoch [22/70], Step [575/655], Loss: 0.1438\n",
      "Epoch [22/70], Step [600/655], Loss: 0.0475\n",
      "Epoch [22/70], Step [625/655], Loss: 0.0679\n",
      "Epoch [22/70], Step [650/655], Loss: 0.0315\n",
      "Start validation #22\n",
      "Validation #22  Average Loss: 1.4034, mIoU: 0.0708\n",
      "Epoch [23/70], Step [25/655], Loss: 0.0759\n",
      "Epoch [23/70], Step [50/655], Loss: 0.0722\n",
      "Epoch [23/70], Step [75/655], Loss: 0.0467\n",
      "Epoch [23/70], Step [100/655], Loss: 0.0714\n",
      "Epoch [23/70], Step [125/655], Loss: 0.0394\n",
      "Epoch [23/70], Step [150/655], Loss: 0.0622\n",
      "Epoch [23/70], Step [175/655], Loss: 0.0683\n",
      "Epoch [23/70], Step [200/655], Loss: 0.0587\n",
      "Epoch [23/70], Step [225/655], Loss: 0.0516\n",
      "Epoch [23/70], Step [250/655], Loss: 0.0626\n",
      "Epoch [23/70], Step [275/655], Loss: 0.1132\n",
      "Epoch [23/70], Step [300/655], Loss: 0.0611\n",
      "Epoch [23/70], Step [325/655], Loss: 0.1198\n",
      "Epoch [23/70], Step [350/655], Loss: 0.0399\n",
      "Epoch [23/70], Step [375/655], Loss: 0.0442\n",
      "Epoch [23/70], Step [400/655], Loss: 0.0815\n",
      "Epoch [23/70], Step [425/655], Loss: 0.0814\n",
      "Epoch [23/70], Step [450/655], Loss: 0.0819\n",
      "Epoch [23/70], Step [475/655], Loss: 0.0502\n",
      "Epoch [23/70], Step [500/655], Loss: 0.0918\n",
      "Epoch [23/70], Step [525/655], Loss: 0.0787\n",
      "Epoch [23/70], Step [550/655], Loss: 0.0718\n",
      "Epoch [23/70], Step [575/655], Loss: 0.0686\n",
      "Epoch [23/70], Step [600/655], Loss: 0.0963\n",
      "Epoch [23/70], Step [625/655], Loss: 0.0366\n",
      "Epoch [23/70], Step [650/655], Loss: 0.0476\n",
      "Start validation #23\n",
      "Validation #23  Average Loss: 1.8163, mIoU: 0.0589\n",
      "Epoch [24/70], Step [25/655], Loss: 0.0716\n",
      "Epoch [24/70], Step [50/655], Loss: 0.0576\n",
      "Epoch [24/70], Step [75/655], Loss: 0.0838\n",
      "Epoch [24/70], Step [100/655], Loss: 0.0572\n",
      "Epoch [24/70], Step [125/655], Loss: 0.0630\n",
      "Epoch [24/70], Step [150/655], Loss: 0.1079\n",
      "Epoch [24/70], Step [175/655], Loss: 0.1876\n",
      "Epoch [24/70], Step [200/655], Loss: 0.0846\n",
      "Epoch [24/70], Step [225/655], Loss: 0.0479\n",
      "Epoch [24/70], Step [250/655], Loss: 0.0667\n",
      "Epoch [24/70], Step [275/655], Loss: 0.0633\n",
      "Epoch [24/70], Step [300/655], Loss: 0.0689\n",
      "Epoch [24/70], Step [325/655], Loss: 0.0468\n",
      "Epoch [24/70], Step [350/655], Loss: 0.0421\n",
      "Epoch [24/70], Step [375/655], Loss: 0.0983\n",
      "Epoch [24/70], Step [400/655], Loss: 0.0992\n",
      "Epoch [24/70], Step [425/655], Loss: 0.1565\n",
      "Epoch [24/70], Step [450/655], Loss: 0.0791\n",
      "Epoch [24/70], Step [475/655], Loss: 0.0398\n",
      "Epoch [24/70], Step [500/655], Loss: 0.0385\n",
      "Epoch [24/70], Step [525/655], Loss: 0.0525\n",
      "Epoch [24/70], Step [550/655], Loss: 0.0731\n",
      "Epoch [24/70], Step [575/655], Loss: 0.1024\n",
      "Epoch [24/70], Step [600/655], Loss: 0.0585\n",
      "Epoch [24/70], Step [625/655], Loss: 0.0745\n",
      "Epoch [24/70], Step [650/655], Loss: 0.0578\n",
      "Start validation #24\n",
      "Validation #24  Average Loss: 1.3031, mIoU: 0.0848\n",
      "Epoch [25/70], Step [25/655], Loss: 0.0947\n",
      "Epoch [25/70], Step [50/655], Loss: 0.0550\n",
      "Epoch [25/70], Step [75/655], Loss: 0.1179\n",
      "Epoch [25/70], Step [100/655], Loss: 0.0822\n",
      "Epoch [25/70], Step [125/655], Loss: 0.0266\n",
      "Epoch [25/70], Step [150/655], Loss: 0.1228\n",
      "Epoch [25/70], Step [175/655], Loss: 0.0359\n",
      "Epoch [25/70], Step [200/655], Loss: 0.0718\n",
      "Epoch [25/70], Step [225/655], Loss: 0.0785\n",
      "Epoch [25/70], Step [250/655], Loss: 0.0420\n",
      "Epoch [25/70], Step [275/655], Loss: 0.0402\n",
      "Epoch [25/70], Step [300/655], Loss: 0.0941\n",
      "Epoch [25/70], Step [325/655], Loss: 0.0267\n",
      "Epoch [25/70], Step [350/655], Loss: 0.0457\n",
      "Epoch [25/70], Step [375/655], Loss: 0.1534\n",
      "Epoch [25/70], Step [400/655], Loss: 0.0917\n",
      "Epoch [25/70], Step [425/655], Loss: 0.0355\n",
      "Epoch [25/70], Step [450/655], Loss: 0.0583\n",
      "Epoch [25/70], Step [475/655], Loss: 0.0720\n",
      "Epoch [25/70], Step [500/655], Loss: 0.0792\n",
      "Epoch [25/70], Step [525/655], Loss: 0.0202\n",
      "Epoch [25/70], Step [550/655], Loss: 0.0428\n",
      "Epoch [25/70], Step [575/655], Loss: 0.0341\n",
      "Epoch [25/70], Step [600/655], Loss: 0.0787\n",
      "Epoch [25/70], Step [625/655], Loss: 0.0713\n",
      "Epoch [25/70], Step [650/655], Loss: 0.0578\n",
      "Start validation #25\n",
      "Validation #25  Average Loss: 0.9972, mIoU: 0.1335\n",
      "Epoch [26/70], Step [25/655], Loss: 0.0906\n",
      "Epoch [26/70], Step [50/655], Loss: 0.0739\n",
      "Epoch [26/70], Step [75/655], Loss: 0.0525\n",
      "Epoch [26/70], Step [100/655], Loss: 0.0550\n",
      "Epoch [26/70], Step [125/655], Loss: 0.0539\n",
      "Epoch [26/70], Step [150/655], Loss: 0.0410\n",
      "Epoch [26/70], Step [175/655], Loss: 0.0794\n",
      "Epoch [26/70], Step [200/655], Loss: 0.0518\n",
      "Epoch [26/70], Step [225/655], Loss: 0.1164\n",
      "Epoch [26/70], Step [250/655], Loss: 0.0730\n",
      "Epoch [26/70], Step [275/655], Loss: 0.0910\n",
      "Epoch [26/70], Step [300/655], Loss: 0.0841\n",
      "Epoch [26/70], Step [325/655], Loss: 0.0665\n",
      "Epoch [26/70], Step [350/655], Loss: 0.0306\n",
      "Epoch [26/70], Step [375/655], Loss: 0.0548\n",
      "Epoch [26/70], Step [400/655], Loss: 0.1474\n",
      "Epoch [26/70], Step [425/655], Loss: 0.0640\n",
      "Epoch [26/70], Step [450/655], Loss: 0.1238\n",
      "Epoch [26/70], Step [475/655], Loss: 0.0686\n",
      "Epoch [26/70], Step [500/655], Loss: 0.0515\n",
      "Epoch [26/70], Step [525/655], Loss: 0.0519\n",
      "Epoch [26/70], Step [550/655], Loss: 0.0690\n",
      "Epoch [26/70], Step [575/655], Loss: 0.0698\n",
      "Epoch [26/70], Step [600/655], Loss: 0.0284\n",
      "Epoch [26/70], Step [625/655], Loss: 0.0832\n",
      "Epoch [26/70], Step [650/655], Loss: 0.0896\n",
      "Start validation #26\n",
      "Validation #26  Average Loss: 0.8181, mIoU: 0.2019\n",
      "Best performance at epoch: 26\n",
      "Save model in ./saved\n",
      "Epoch [27/70], Step [25/655], Loss: 0.0802\n",
      "Epoch [27/70], Step [50/655], Loss: 0.0848\n",
      "Epoch [27/70], Step [75/655], Loss: 0.1062\n",
      "Epoch [27/70], Step [100/655], Loss: 0.1215\n",
      "Epoch [27/70], Step [125/655], Loss: 0.0527\n",
      "Epoch [27/70], Step [150/655], Loss: 0.0318\n",
      "Epoch [27/70], Step [175/655], Loss: 0.0416\n",
      "Epoch [27/70], Step [200/655], Loss: 0.0951\n",
      "Epoch [27/70], Step [225/655], Loss: 0.0476\n",
      "Epoch [27/70], Step [250/655], Loss: 0.0726\n",
      "Epoch [27/70], Step [275/655], Loss: 0.0896\n",
      "Epoch [27/70], Step [300/655], Loss: 0.0316\n",
      "Epoch [27/70], Step [325/655], Loss: 0.0459\n",
      "Epoch [27/70], Step [350/655], Loss: 0.0516\n",
      "Epoch [27/70], Step [375/655], Loss: 0.0533\n",
      "Epoch [27/70], Step [400/655], Loss: 0.1180\n",
      "Epoch [27/70], Step [425/655], Loss: 0.1867\n",
      "Epoch [27/70], Step [450/655], Loss: 0.0414\n",
      "Epoch [27/70], Step [475/655], Loss: 0.1114\n",
      "Epoch [27/70], Step [500/655], Loss: 0.0287\n",
      "Epoch [27/70], Step [525/655], Loss: 0.0502\n",
      "Epoch [27/70], Step [550/655], Loss: 0.0625\n",
      "Epoch [27/70], Step [575/655], Loss: 0.0697\n",
      "Epoch [27/70], Step [600/655], Loss: 0.0499\n",
      "Epoch [27/70], Step [625/655], Loss: 0.1005\n",
      "Epoch [27/70], Step [650/655], Loss: 0.0486\n",
      "Start validation #27\n",
      "Validation #27  Average Loss: 1.0996, mIoU: 0.1364\n",
      "Epoch [28/70], Step [25/655], Loss: 0.0335\n",
      "Epoch [28/70], Step [50/655], Loss: 0.0686\n",
      "Epoch [28/70], Step [75/655], Loss: 0.0490\n",
      "Epoch [28/70], Step [100/655], Loss: 0.0851\n",
      "Epoch [28/70], Step [125/655], Loss: 0.0738\n",
      "Epoch [28/70], Step [150/655], Loss: 0.0427\n",
      "Epoch [28/70], Step [175/655], Loss: 0.0679\n",
      "Epoch [28/70], Step [200/655], Loss: 0.1220\n",
      "Epoch [28/70], Step [225/655], Loss: 0.0361\n",
      "Epoch [28/70], Step [250/655], Loss: 0.0578\n",
      "Epoch [28/70], Step [275/655], Loss: 0.1073\n",
      "Epoch [28/70], Step [300/655], Loss: 0.1177\n",
      "Epoch [28/70], Step [325/655], Loss: 0.0358\n",
      "Epoch [28/70], Step [350/655], Loss: 0.0633\n",
      "Epoch [28/70], Step [375/655], Loss: 0.0463\n",
      "Epoch [28/70], Step [400/655], Loss: 0.0290\n",
      "Epoch [28/70], Step [425/655], Loss: 0.0855\n",
      "Epoch [28/70], Step [450/655], Loss: 0.0919\n",
      "Epoch [28/70], Step [475/655], Loss: 0.1664\n",
      "Epoch [28/70], Step [500/655], Loss: 0.0304\n",
      "Epoch [28/70], Step [525/655], Loss: 0.0398\n",
      "Epoch [28/70], Step [550/655], Loss: 0.0509\n",
      "Epoch [28/70], Step [575/655], Loss: 0.0635\n",
      "Epoch [28/70], Step [600/655], Loss: 0.0331\n",
      "Epoch [28/70], Step [625/655], Loss: 0.0292\n",
      "Epoch [28/70], Step [650/655], Loss: 0.0776\n",
      "Start validation #28\n",
      "Validation #28  Average Loss: 1.2698, mIoU: 0.1081\n",
      "Epoch [29/70], Step [25/655], Loss: 0.0661\n",
      "Epoch [29/70], Step [50/655], Loss: 0.1305\n",
      "Epoch [29/70], Step [75/655], Loss: 0.0502\n",
      "Epoch [29/70], Step [100/655], Loss: 0.0388\n",
      "Epoch [29/70], Step [125/655], Loss: 0.0690\n",
      "Epoch [29/70], Step [150/655], Loss: 0.0619\n",
      "Epoch [29/70], Step [175/655], Loss: 0.0672\n",
      "Epoch [29/70], Step [200/655], Loss: 0.0474\n",
      "Epoch [29/70], Step [225/655], Loss: 0.0745\n",
      "Epoch [29/70], Step [250/655], Loss: 0.0430\n",
      "Epoch [29/70], Step [275/655], Loss: 0.0584\n",
      "Epoch [29/70], Step [300/655], Loss: 0.0356\n",
      "Epoch [29/70], Step [325/655], Loss: 0.0389\n",
      "Epoch [29/70], Step [350/655], Loss: 0.0362\n",
      "Epoch [29/70], Step [375/655], Loss: 0.0614\n",
      "Epoch [29/70], Step [400/655], Loss: 0.0313\n",
      "Epoch [29/70], Step [425/655], Loss: 0.0492\n",
      "Epoch [29/70], Step [450/655], Loss: 0.1122\n",
      "Epoch [29/70], Step [475/655], Loss: 0.0348\n",
      "Epoch [29/70], Step [500/655], Loss: 0.0580\n",
      "Epoch [29/70], Step [525/655], Loss: 0.0302\n",
      "Epoch [29/70], Step [550/655], Loss: 0.0384\n",
      "Epoch [29/70], Step [575/655], Loss: 0.0981\n",
      "Epoch [29/70], Step [600/655], Loss: 0.1283\n",
      "Epoch [29/70], Step [625/655], Loss: 0.0540\n",
      "Epoch [29/70], Step [650/655], Loss: 0.0625\n",
      "Start validation #29\n",
      "Validation #29  Average Loss: 1.4188, mIoU: 0.0959\n",
      "Epoch [30/70], Step [25/655], Loss: 0.0594\n",
      "Epoch [30/70], Step [50/655], Loss: 0.0614\n",
      "Epoch [30/70], Step [75/655], Loss: 0.0572\n",
      "Epoch [30/70], Step [100/655], Loss: 0.0559\n",
      "Epoch [30/70], Step [125/655], Loss: 0.0558\n",
      "Epoch [30/70], Step [150/655], Loss: 0.1052\n",
      "Epoch [30/70], Step [175/655], Loss: 0.0520\n",
      "Epoch [30/70], Step [200/655], Loss: 0.0481\n",
      "Epoch [30/70], Step [225/655], Loss: 0.1200\n",
      "Epoch [30/70], Step [250/655], Loss: 0.0448\n",
      "Epoch [30/70], Step [275/655], Loss: 0.0484\n",
      "Epoch [30/70], Step [300/655], Loss: 0.0453\n",
      "Epoch [30/70], Step [325/655], Loss: 0.0351\n",
      "Epoch [30/70], Step [350/655], Loss: 0.0413\n",
      "Epoch [30/70], Step [375/655], Loss: 0.0303\n",
      "Epoch [30/70], Step [400/655], Loss: 0.0229\n",
      "Epoch [30/70], Step [425/655], Loss: 0.0512\n",
      "Epoch [30/70], Step [450/655], Loss: 0.0551\n",
      "Epoch [30/70], Step [475/655], Loss: 0.0917\n",
      "Epoch [30/70], Step [500/655], Loss: 0.0535\n",
      "Epoch [30/70], Step [525/655], Loss: 0.0682\n",
      "Epoch [30/70], Step [550/655], Loss: 0.0346\n",
      "Epoch [30/70], Step [575/655], Loss: 0.0624\n",
      "Epoch [30/70], Step [600/655], Loss: 0.0463\n",
      "Epoch [30/70], Step [625/655], Loss: 0.0215\n",
      "Epoch [30/70], Step [650/655], Loss: 0.0371\n",
      "Start validation #30\n",
      "Validation #30  Average Loss: 0.8066, mIoU: 0.2157\n",
      "Best performance at epoch: 30\n",
      "Save model in ./saved\n",
      "Epoch [31/70], Step [25/655], Loss: 0.0632\n",
      "Epoch [31/70], Step [50/655], Loss: 0.1046\n",
      "Epoch [31/70], Step [75/655], Loss: 0.0308\n",
      "Epoch [31/70], Step [100/655], Loss: 0.0343\n",
      "Epoch [31/70], Step [125/655], Loss: 0.0485\n",
      "Epoch [31/70], Step [150/655], Loss: 0.0555\n",
      "Epoch [31/70], Step [175/655], Loss: 0.0498\n",
      "Epoch [31/70], Step [200/655], Loss: 0.0236\n",
      "Epoch [31/70], Step [225/655], Loss: 0.0560\n",
      "Epoch [31/70], Step [250/655], Loss: 0.0379\n",
      "Epoch [31/70], Step [275/655], Loss: 0.0638\n",
      "Epoch [31/70], Step [300/655], Loss: 0.0388\n",
      "Epoch [31/70], Step [325/655], Loss: 0.0401\n",
      "Epoch [31/70], Step [350/655], Loss: 0.0291\n",
      "Epoch [31/70], Step [375/655], Loss: 0.0322\n",
      "Epoch [31/70], Step [400/655], Loss: 0.0750\n",
      "Epoch [31/70], Step [425/655], Loss: 0.0666\n",
      "Epoch [31/70], Step [450/655], Loss: 0.0543\n",
      "Epoch [31/70], Step [475/655], Loss: 0.0484\n",
      "Epoch [31/70], Step [500/655], Loss: 0.0336\n",
      "Epoch [31/70], Step [525/655], Loss: 0.0486\n",
      "Epoch [31/70], Step [550/655], Loss: 0.0611\n",
      "Epoch [31/70], Step [575/655], Loss: 0.0621\n",
      "Epoch [31/70], Step [600/655], Loss: 0.0697\n",
      "Epoch [31/70], Step [625/655], Loss: 0.0730\n",
      "Epoch [31/70], Step [650/655], Loss: 0.0343\n",
      "Start validation #31\n",
      "Validation #31  Average Loss: 1.5256, mIoU: 0.0976\n",
      "Epoch [32/70], Step [25/655], Loss: 0.0483\n",
      "Epoch [32/70], Step [50/655], Loss: 0.0586\n",
      "Epoch [32/70], Step [75/655], Loss: 0.0413\n",
      "Epoch [32/70], Step [100/655], Loss: 0.0236\n",
      "Epoch [32/70], Step [125/655], Loss: 0.0736\n",
      "Epoch [32/70], Step [150/655], Loss: 0.0377\n",
      "Epoch [32/70], Step [175/655], Loss: 0.0603\n",
      "Epoch [32/70], Step [200/655], Loss: 0.0422\n",
      "Epoch [32/70], Step [225/655], Loss: 0.0280\n",
      "Epoch [32/70], Step [250/655], Loss: 0.0933\n",
      "Epoch [32/70], Step [275/655], Loss: 0.0262\n",
      "Epoch [32/70], Step [300/655], Loss: 0.0331\n",
      "Epoch [32/70], Step [325/655], Loss: 0.0593\n",
      "Epoch [32/70], Step [350/655], Loss: 0.0267\n",
      "Epoch [32/70], Step [375/655], Loss: 0.0482\n",
      "Epoch [32/70], Step [400/655], Loss: 0.0385\n",
      "Epoch [32/70], Step [425/655], Loss: 0.0231\n",
      "Epoch [32/70], Step [450/655], Loss: 0.0208\n",
      "Epoch [32/70], Step [475/655], Loss: 0.0535\n",
      "Epoch [32/70], Step [500/655], Loss: 0.0963\n",
      "Epoch [32/70], Step [525/655], Loss: 0.0451\n",
      "Epoch [32/70], Step [550/655], Loss: 0.0605\n",
      "Epoch [32/70], Step [575/655], Loss: 0.0521\n",
      "Epoch [32/70], Step [600/655], Loss: 0.0314\n",
      "Epoch [32/70], Step [625/655], Loss: 0.0537\n",
      "Epoch [32/70], Step [650/655], Loss: 0.1123\n",
      "Start validation #32\n",
      "Validation #32  Average Loss: 1.9271, mIoU: 0.0659\n",
      "Epoch [33/70], Step [25/655], Loss: 0.0644\n",
      "Epoch [33/70], Step [50/655], Loss: 0.0522\n",
      "Epoch [33/70], Step [75/655], Loss: 0.0180\n",
      "Epoch [33/70], Step [100/655], Loss: 0.0418\n",
      "Epoch [33/70], Step [125/655], Loss: 0.0458\n",
      "Epoch [33/70], Step [150/655], Loss: 0.0872\n",
      "Epoch [33/70], Step [175/655], Loss: 0.0400\n",
      "Epoch [33/70], Step [200/655], Loss: 0.0402\n",
      "Epoch [33/70], Step [225/655], Loss: 0.0473\n",
      "Epoch [33/70], Step [250/655], Loss: 0.0306\n",
      "Epoch [33/70], Step [275/655], Loss: 0.0338\n",
      "Epoch [33/70], Step [300/655], Loss: 0.0659\n",
      "Epoch [33/70], Step [325/655], Loss: 0.0250\n",
      "Epoch [33/70], Step [350/655], Loss: 0.0501\n",
      "Epoch [33/70], Step [375/655], Loss: 0.0253\n",
      "Epoch [33/70], Step [400/655], Loss: 0.0507\n",
      "Epoch [33/70], Step [425/655], Loss: 0.0394\n",
      "Epoch [33/70], Step [450/655], Loss: 0.0486\n",
      "Epoch [33/70], Step [475/655], Loss: 0.0701\n",
      "Epoch [33/70], Step [500/655], Loss: 0.0559\n",
      "Epoch [33/70], Step [525/655], Loss: 0.0280\n",
      "Epoch [33/70], Step [550/655], Loss: 0.0279\n",
      "Epoch [33/70], Step [575/655], Loss: 0.0494\n",
      "Epoch [33/70], Step [600/655], Loss: 0.0544\n",
      "Epoch [33/70], Step [625/655], Loss: 0.0216\n",
      "Epoch [33/70], Step [650/655], Loss: 0.0511\n",
      "Start validation #33\n",
      "Validation #33  Average Loss: 1.3141, mIoU: 0.1206\n",
      "Epoch [34/70], Step [25/655], Loss: 0.0411\n",
      "Epoch [34/70], Step [50/655], Loss: 0.0589\n",
      "Epoch [34/70], Step [75/655], Loss: 0.0473\n",
      "Epoch [34/70], Step [100/655], Loss: 0.0326\n",
      "Epoch [34/70], Step [125/655], Loss: 0.0390\n",
      "Epoch [34/70], Step [150/655], Loss: 0.0510\n",
      "Epoch [34/70], Step [175/655], Loss: 0.0504\n",
      "Epoch [34/70], Step [200/655], Loss: 0.0677\n",
      "Epoch [34/70], Step [225/655], Loss: 0.0247\n",
      "Epoch [34/70], Step [250/655], Loss: 0.0990\n",
      "Epoch [34/70], Step [275/655], Loss: 0.0431\n",
      "Epoch [34/70], Step [300/655], Loss: 0.0674\n",
      "Epoch [34/70], Step [325/655], Loss: 0.0714\n",
      "Epoch [34/70], Step [350/655], Loss: 0.0711\n",
      "Epoch [34/70], Step [375/655], Loss: 0.0521\n",
      "Epoch [34/70], Step [400/655], Loss: 0.0315\n",
      "Epoch [34/70], Step [425/655], Loss: 0.0387\n",
      "Epoch [34/70], Step [450/655], Loss: 0.0252\n",
      "Epoch [34/70], Step [475/655], Loss: 0.0444\n",
      "Epoch [34/70], Step [500/655], Loss: 0.0811\n",
      "Epoch [34/70], Step [525/655], Loss: 0.0716\n",
      "Epoch [34/70], Step [550/655], Loss: 0.0775\n",
      "Epoch [34/70], Step [575/655], Loss: 0.0216\n",
      "Epoch [34/70], Step [600/655], Loss: 0.0532\n",
      "Epoch [34/70], Step [625/655], Loss: 0.0224\n",
      "Epoch [34/70], Step [650/655], Loss: 0.0435\n",
      "Start validation #34\n",
      "Validation #34  Average Loss: 0.8041, mIoU: 0.2134\n",
      "Epoch [35/70], Step [25/655], Loss: 0.0205\n",
      "Epoch [35/70], Step [50/655], Loss: 0.0156\n",
      "Epoch [35/70], Step [75/655], Loss: 0.0481\n",
      "Epoch [35/70], Step [100/655], Loss: 0.0251\n",
      "Epoch [35/70], Step [125/655], Loss: 0.0626\n",
      "Epoch [35/70], Step [150/655], Loss: 0.0440\n",
      "Epoch [35/70], Step [175/655], Loss: 0.0507\n",
      "Epoch [35/70], Step [200/655], Loss: 0.0522\n",
      "Epoch [35/70], Step [225/655], Loss: 0.0439\n",
      "Epoch [35/70], Step [250/655], Loss: 0.0335\n",
      "Epoch [35/70], Step [275/655], Loss: 0.0441\n",
      "Epoch [35/70], Step [300/655], Loss: 0.0292\n",
      "Epoch [35/70], Step [325/655], Loss: 0.0340\n",
      "Epoch [35/70], Step [350/655], Loss: 0.0273\n",
      "Epoch [35/70], Step [375/655], Loss: 0.1191\n",
      "Epoch [35/70], Step [400/655], Loss: 0.0421\n",
      "Epoch [35/70], Step [425/655], Loss: 0.0384\n",
      "Epoch [35/70], Step [450/655], Loss: 0.0245\n",
      "Epoch [35/70], Step [475/655], Loss: 0.0472\n",
      "Epoch [35/70], Step [500/655], Loss: 0.0257\n",
      "Epoch [35/70], Step [525/655], Loss: 0.0465\n",
      "Epoch [35/70], Step [550/655], Loss: 0.0258\n",
      "Epoch [35/70], Step [575/655], Loss: 0.0322\n",
      "Epoch [35/70], Step [600/655], Loss: 0.0755\n",
      "Epoch [35/70], Step [625/655], Loss: 0.0667\n",
      "Epoch [35/70], Step [650/655], Loss: 0.0638\n",
      "Start validation #35\n",
      "Validation #35  Average Loss: 0.9413, mIoU: 0.1533\n",
      "Epoch [36/70], Step [25/655], Loss: 0.0438\n",
      "Epoch [36/70], Step [50/655], Loss: 0.0496\n",
      "Epoch [36/70], Step [75/655], Loss: 0.0928\n",
      "Epoch [36/70], Step [100/655], Loss: 0.0531\n",
      "Epoch [36/70], Step [125/655], Loss: 0.0597\n",
      "Epoch [36/70], Step [150/655], Loss: 0.0322\n",
      "Epoch [36/70], Step [175/655], Loss: 0.0347\n",
      "Epoch [36/70], Step [200/655], Loss: 0.0259\n",
      "Epoch [36/70], Step [225/655], Loss: 0.0377\n",
      "Epoch [36/70], Step [250/655], Loss: 0.0525\n",
      "Epoch [36/70], Step [275/655], Loss: 0.0608\n",
      "Epoch [36/70], Step [300/655], Loss: 0.0183\n",
      "Epoch [36/70], Step [325/655], Loss: 0.0476\n",
      "Epoch [36/70], Step [350/655], Loss: 0.0273\n",
      "Epoch [36/70], Step [375/655], Loss: 0.0557\n",
      "Epoch [36/70], Step [400/655], Loss: 0.0650\n",
      "Epoch [36/70], Step [425/655], Loss: 0.0624\n",
      "Epoch [36/70], Step [450/655], Loss: 0.0439\n",
      "Epoch [36/70], Step [475/655], Loss: 0.0175\n",
      "Epoch [36/70], Step [500/655], Loss: 0.0334\n",
      "Epoch [36/70], Step [525/655], Loss: 0.0385\n",
      "Epoch [36/70], Step [550/655], Loss: 0.0957\n",
      "Epoch [36/70], Step [575/655], Loss: 0.0285\n",
      "Epoch [36/70], Step [600/655], Loss: 0.0559\n",
      "Epoch [36/70], Step [625/655], Loss: 0.0279\n",
      "Epoch [36/70], Step [650/655], Loss: 0.0325\n",
      "Start validation #36\n",
      "Validation #36  Average Loss: 0.8126, mIoU: 0.2616\n",
      "Best performance at epoch: 36\n",
      "Save model in ./saved\n",
      "Epoch [37/70], Step [25/655], Loss: 0.0434\n",
      "Epoch [37/70], Step [50/655], Loss: 0.0284\n",
      "Epoch [37/70], Step [75/655], Loss: 0.0671\n",
      "Epoch [37/70], Step [100/655], Loss: 0.0265\n",
      "Epoch [37/70], Step [125/655], Loss: 0.0251\n",
      "Epoch [37/70], Step [150/655], Loss: 0.0383\n",
      "Epoch [37/70], Step [175/655], Loss: 0.0489\n",
      "Epoch [37/70], Step [200/655], Loss: 0.0512\n",
      "Epoch [37/70], Step [225/655], Loss: 0.0442\n",
      "Epoch [37/70], Step [250/655], Loss: 0.0679\n",
      "Epoch [37/70], Step [275/655], Loss: 0.0633\n",
      "Epoch [37/70], Step [300/655], Loss: 0.0931\n",
      "Epoch [37/70], Step [325/655], Loss: 0.0913\n",
      "Epoch [37/70], Step [350/655], Loss: 0.0635\n",
      "Epoch [37/70], Step [375/655], Loss: 0.0630\n",
      "Epoch [37/70], Step [400/655], Loss: 0.0346\n",
      "Epoch [37/70], Step [425/655], Loss: 0.0705\n",
      "Epoch [37/70], Step [450/655], Loss: 0.0208\n",
      "Epoch [37/70], Step [475/655], Loss: 0.0263\n",
      "Epoch [37/70], Step [500/655], Loss: 0.0396\n",
      "Epoch [37/70], Step [525/655], Loss: 0.0746\n",
      "Epoch [37/70], Step [550/655], Loss: 0.0314\n",
      "Epoch [37/70], Step [575/655], Loss: 0.0344\n",
      "Epoch [37/70], Step [600/655], Loss: 0.0594\n",
      "Epoch [37/70], Step [625/655], Loss: 0.0313\n",
      "Epoch [37/70], Step [650/655], Loss: 0.0355\n",
      "Start validation #37\n",
      "Validation #37  Average Loss: 1.0738, mIoU: 0.1749\n",
      "Epoch [38/70], Step [25/655], Loss: 0.0406\n",
      "Epoch [38/70], Step [50/655], Loss: 0.0430\n",
      "Epoch [38/70], Step [75/655], Loss: 0.0344\n",
      "Epoch [38/70], Step [100/655], Loss: 0.0228\n",
      "Epoch [38/70], Step [125/655], Loss: 0.0236\n",
      "Epoch [38/70], Step [150/655], Loss: 0.0360\n",
      "Epoch [38/70], Step [175/655], Loss: 0.0363\n",
      "Epoch [38/70], Step [200/655], Loss: 0.0614\n",
      "Epoch [38/70], Step [225/655], Loss: 0.0298\n",
      "Epoch [38/70], Step [250/655], Loss: 0.0345\n",
      "Epoch [38/70], Step [275/655], Loss: 0.0647\n",
      "Epoch [38/70], Step [300/655], Loss: 0.0406\n",
      "Epoch [38/70], Step [325/655], Loss: 0.0244\n",
      "Epoch [38/70], Step [350/655], Loss: 0.0292\n",
      "Epoch [38/70], Step [375/655], Loss: 0.0304\n",
      "Epoch [38/70], Step [400/655], Loss: 0.0342\n",
      "Epoch [38/70], Step [425/655], Loss: 0.0622\n",
      "Epoch [38/70], Step [450/655], Loss: 0.0722\n",
      "Epoch [38/70], Step [475/655], Loss: 0.1107\n",
      "Epoch [38/70], Step [500/655], Loss: 0.0488\n",
      "Epoch [38/70], Step [525/655], Loss: 0.0818\n",
      "Epoch [38/70], Step [550/655], Loss: 0.0582\n",
      "Epoch [38/70], Step [575/655], Loss: 0.0314\n",
      "Epoch [38/70], Step [600/655], Loss: 0.0240\n",
      "Epoch [38/70], Step [625/655], Loss: 0.0397\n",
      "Epoch [38/70], Step [650/655], Loss: 0.0497\n",
      "Start validation #38\n",
      "Validation #38  Average Loss: 0.6324, mIoU: 0.2968\n",
      "Best performance at epoch: 38\n",
      "Save model in ./saved\n",
      "Epoch [39/70], Step [25/655], Loss: 0.0347\n",
      "Epoch [39/70], Step [50/655], Loss: 0.1088\n",
      "Epoch [39/70], Step [75/655], Loss: 0.0396\n",
      "Epoch [39/70], Step [100/655], Loss: 0.0458\n",
      "Epoch [39/70], Step [125/655], Loss: 0.0381\n",
      "Epoch [39/70], Step [150/655], Loss: 0.0730\n",
      "Epoch [39/70], Step [175/655], Loss: 0.0473\n",
      "Epoch [39/70], Step [200/655], Loss: 0.0275\n",
      "Epoch [39/70], Step [225/655], Loss: 0.1239\n",
      "Epoch [39/70], Step [250/655], Loss: 0.0576\n",
      "Epoch [39/70], Step [275/655], Loss: 0.0579\n",
      "Epoch [39/70], Step [300/655], Loss: 0.0509\n",
      "Epoch [39/70], Step [325/655], Loss: 0.0758\n",
      "Epoch [39/70], Step [350/655], Loss: 0.0205\n",
      "Epoch [39/70], Step [375/655], Loss: 0.0411\n",
      "Epoch [39/70], Step [400/655], Loss: 0.0305\n",
      "Epoch [39/70], Step [425/655], Loss: 0.0343\n",
      "Epoch [39/70], Step [450/655], Loss: 0.0389\n",
      "Epoch [39/70], Step [475/655], Loss: 0.0236\n",
      "Epoch [39/70], Step [500/655], Loss: 0.0888\n",
      "Epoch [39/70], Step [525/655], Loss: 0.0572\n",
      "Epoch [39/70], Step [550/655], Loss: 0.1121\n",
      "Epoch [39/70], Step [575/655], Loss: 0.0599\n",
      "Epoch [39/70], Step [600/655], Loss: 0.0353\n",
      "Epoch [39/70], Step [625/655], Loss: 0.0387\n",
      "Epoch [39/70], Step [650/655], Loss: 0.0480\n",
      "Start validation #39\n",
      "Validation #39  Average Loss: 0.8851, mIoU: 0.2082\n",
      "Epoch [40/70], Step [25/655], Loss: 0.0765\n",
      "Epoch [40/70], Step [50/655], Loss: 0.0450\n",
      "Epoch [40/70], Step [75/655], Loss: 0.0560\n",
      "Epoch [40/70], Step [100/655], Loss: 0.0221\n",
      "Epoch [40/70], Step [125/655], Loss: 0.0498\n",
      "Epoch [40/70], Step [150/655], Loss: 0.0642\n",
      "Epoch [40/70], Step [175/655], Loss: 0.0503\n",
      "Epoch [40/70], Step [200/655], Loss: 0.0259\n",
      "Epoch [40/70], Step [225/655], Loss: 0.0352\n",
      "Epoch [40/70], Step [250/655], Loss: 0.0147\n",
      "Epoch [40/70], Step [275/655], Loss: 0.0240\n",
      "Epoch [40/70], Step [300/655], Loss: 0.0326\n",
      "Epoch [40/70], Step [325/655], Loss: 0.0256\n",
      "Epoch [40/70], Step [350/655], Loss: 0.0326\n",
      "Epoch [40/70], Step [375/655], Loss: 0.0199\n",
      "Epoch [40/70], Step [400/655], Loss: 0.0161\n",
      "Epoch [40/70], Step [425/655], Loss: 0.0454\n",
      "Epoch [40/70], Step [450/655], Loss: 0.0759\n",
      "Epoch [40/70], Step [475/655], Loss: 0.0330\n",
      "Epoch [40/70], Step [500/655], Loss: 0.0205\n",
      "Epoch [40/70], Step [525/655], Loss: 0.0289\n",
      "Epoch [40/70], Step [550/655], Loss: 0.0392\n",
      "Epoch [40/70], Step [575/655], Loss: 0.0356\n",
      "Epoch [40/70], Step [600/655], Loss: 0.0310\n",
      "Epoch [40/70], Step [625/655], Loss: 0.0275\n",
      "Epoch [40/70], Step [650/655], Loss: 0.0576\n",
      "Start validation #40\n",
      "Validation #40  Average Loss: 0.5675, mIoU: 0.3611\n",
      "Best performance at epoch: 40\n",
      "Save model in ./saved\n",
      "Epoch [41/70], Step [25/655], Loss: 0.0272\n",
      "Epoch [41/70], Step [50/655], Loss: 0.0311\n",
      "Epoch [41/70], Step [75/655], Loss: 0.0301\n",
      "Epoch [41/70], Step [100/655], Loss: 0.0310\n",
      "Epoch [41/70], Step [125/655], Loss: 0.0472\n",
      "Epoch [41/70], Step [150/655], Loss: 0.0293\n",
      "Epoch [41/70], Step [175/655], Loss: 0.0542\n",
      "Epoch [41/70], Step [200/655], Loss: 0.0420\n",
      "Epoch [41/70], Step [225/655], Loss: 0.0421\n",
      "Epoch [41/70], Step [250/655], Loss: 0.0679\n",
      "Epoch [41/70], Step [275/655], Loss: 0.0504\n",
      "Epoch [41/70], Step [300/655], Loss: 0.0319\n",
      "Epoch [41/70], Step [325/655], Loss: 0.0271\n",
      "Epoch [41/70], Step [350/655], Loss: 0.0271\n",
      "Epoch [41/70], Step [375/655], Loss: 0.0324\n",
      "Epoch [41/70], Step [400/655], Loss: 0.0317\n",
      "Epoch [41/70], Step [425/655], Loss: 0.1379\n",
      "Epoch [41/70], Step [450/655], Loss: 0.0260\n",
      "Epoch [41/70], Step [475/655], Loss: 0.0559\n",
      "Epoch [41/70], Step [500/655], Loss: 0.0207\n",
      "Epoch [41/70], Step [525/655], Loss: 0.0216\n",
      "Epoch [41/70], Step [550/655], Loss: 0.0186\n",
      "Epoch [41/70], Step [575/655], Loss: 0.0414\n",
      "Epoch [41/70], Step [600/655], Loss: 0.0363\n",
      "Epoch [41/70], Step [625/655], Loss: 0.0460\n",
      "Epoch [41/70], Step [650/655], Loss: 0.0303\n",
      "Start validation #41\n",
      "Validation #41  Average Loss: 0.7793, mIoU: 0.2641\n",
      "Epoch [42/70], Step [25/655], Loss: 0.0401\n",
      "Epoch [42/70], Step [50/655], Loss: 0.0325\n",
      "Epoch [42/70], Step [75/655], Loss: 0.0286\n",
      "Epoch [42/70], Step [100/655], Loss: 0.0249\n",
      "Epoch [42/70], Step [125/655], Loss: 0.0277\n",
      "Epoch [42/70], Step [150/655], Loss: 0.0311\n",
      "Epoch [42/70], Step [175/655], Loss: 0.0462\n",
      "Epoch [42/70], Step [200/655], Loss: 0.0296\n",
      "Epoch [42/70], Step [225/655], Loss: 0.0255\n",
      "Epoch [42/70], Step [250/655], Loss: 0.0342\n",
      "Epoch [42/70], Step [275/655], Loss: 0.0261\n",
      "Epoch [42/70], Step [300/655], Loss: 0.0149\n",
      "Epoch [42/70], Step [325/655], Loss: 0.0405\n",
      "Epoch [42/70], Step [350/655], Loss: 0.1094\n",
      "Epoch [42/70], Step [375/655], Loss: 0.0470\n",
      "Epoch [42/70], Step [400/655], Loss: 0.0297\n",
      "Epoch [42/70], Step [425/655], Loss: 0.0441\n",
      "Epoch [42/70], Step [450/655], Loss: 0.0137\n",
      "Epoch [42/70], Step [475/655], Loss: 0.0419\n",
      "Epoch [42/70], Step [500/655], Loss: 0.0439\n",
      "Epoch [42/70], Step [525/655], Loss: 0.0602\n",
      "Epoch [42/70], Step [550/655], Loss: 0.0600\n",
      "Epoch [42/70], Step [575/655], Loss: 0.0328\n",
      "Epoch [42/70], Step [600/655], Loss: 0.0348\n",
      "Epoch [42/70], Step [625/655], Loss: 0.0297\n",
      "Epoch [42/70], Step [650/655], Loss: 0.0332\n",
      "Start validation #42\n",
      "Validation #42  Average Loss: 1.0206, mIoU: 0.1779\n",
      "Epoch [43/70], Step [25/655], Loss: 0.0392\n",
      "Epoch [43/70], Step [50/655], Loss: 0.0548\n",
      "Epoch [43/70], Step [75/655], Loss: 0.0160\n",
      "Epoch [43/70], Step [100/655], Loss: 0.0334\n",
      "Epoch [43/70], Step [125/655], Loss: 0.0326\n",
      "Epoch [43/70], Step [150/655], Loss: 0.0487\n",
      "Epoch [43/70], Step [175/655], Loss: 0.0369\n",
      "Epoch [43/70], Step [200/655], Loss: 0.0330\n",
      "Epoch [43/70], Step [225/655], Loss: 0.0234\n",
      "Epoch [43/70], Step [250/655], Loss: 0.0342\n",
      "Epoch [43/70], Step [275/655], Loss: 0.0301\n",
      "Epoch [43/70], Step [300/655], Loss: 0.0202\n",
      "Epoch [43/70], Step [325/655], Loss: 0.0162\n",
      "Epoch [43/70], Step [350/655], Loss: 0.0218\n",
      "Epoch [43/70], Step [375/655], Loss: 0.0304\n",
      "Epoch [43/70], Step [400/655], Loss: 0.0447\n",
      "Epoch [43/70], Step [425/655], Loss: 0.0592\n",
      "Epoch [43/70], Step [450/655], Loss: 0.0152\n",
      "Epoch [43/70], Step [475/655], Loss: 0.0226\n",
      "Epoch [43/70], Step [500/655], Loss: 0.0458\n",
      "Epoch [43/70], Step [525/655], Loss: 0.0418\n",
      "Epoch [43/70], Step [550/655], Loss: 0.0281\n",
      "Epoch [43/70], Step [575/655], Loss: 0.0251\n",
      "Epoch [43/70], Step [600/655], Loss: 0.0223\n",
      "Epoch [43/70], Step [625/655], Loss: 0.0191\n",
      "Epoch [43/70], Step [650/655], Loss: 0.0295\n",
      "Start validation #43\n",
      "Validation #43  Average Loss: 2.2545, mIoU: 0.1054\n",
      "Epoch [44/70], Step [25/655], Loss: 0.0602\n",
      "Epoch [44/70], Step [50/655], Loss: 0.0463\n",
      "Epoch [44/70], Step [75/655], Loss: 0.0508\n",
      "Epoch [44/70], Step [100/655], Loss: 0.0541\n",
      "Epoch [44/70], Step [125/655], Loss: 0.0413\n",
      "Epoch [44/70], Step [150/655], Loss: 0.0514\n",
      "Epoch [44/70], Step [175/655], Loss: 0.0391\n",
      "Epoch [44/70], Step [200/655], Loss: 0.0404\n",
      "Epoch [44/70], Step [225/655], Loss: 0.0350\n",
      "Epoch [44/70], Step [250/655], Loss: 0.0260\n",
      "Epoch [44/70], Step [275/655], Loss: 0.0422\n",
      "Epoch [44/70], Step [300/655], Loss: 0.0169\n",
      "Epoch [44/70], Step [325/655], Loss: 0.0371\n",
      "Epoch [44/70], Step [350/655], Loss: 0.0990\n",
      "Epoch [44/70], Step [375/655], Loss: 0.0171\n",
      "Epoch [44/70], Step [400/655], Loss: 0.0448\n",
      "Epoch [44/70], Step [425/655], Loss: 0.0282\n",
      "Epoch [44/70], Step [450/655], Loss: 0.0238\n",
      "Epoch [44/70], Step [475/655], Loss: 0.0387\n",
      "Epoch [44/70], Step [500/655], Loss: 0.0390\n",
      "Epoch [44/70], Step [525/655], Loss: 0.0357\n",
      "Epoch [44/70], Step [550/655], Loss: 0.0344\n",
      "Epoch [44/70], Step [575/655], Loss: 0.0185\n",
      "Epoch [44/70], Step [600/655], Loss: 0.0215\n",
      "Epoch [44/70], Step [625/655], Loss: 0.0614\n",
      "Epoch [44/70], Step [650/655], Loss: 0.0983\n",
      "Start validation #44\n",
      "Validation #44  Average Loss: 2.1516, mIoU: 0.1249\n",
      "Epoch [45/70], Step [25/655], Loss: 0.0165\n",
      "Epoch [45/70], Step [50/655], Loss: 0.0410\n",
      "Epoch [45/70], Step [75/655], Loss: 0.0196\n",
      "Epoch [45/70], Step [100/655], Loss: 0.0547\n",
      "Epoch [45/70], Step [125/655], Loss: 0.0512\n",
      "Epoch [45/70], Step [150/655], Loss: 0.0274\n",
      "Epoch [45/70], Step [175/655], Loss: 0.0260\n",
      "Epoch [45/70], Step [200/655], Loss: 0.0331\n",
      "Epoch [45/70], Step [225/655], Loss: 0.0599\n",
      "Epoch [45/70], Step [250/655], Loss: 0.0426\n",
      "Epoch [45/70], Step [275/655], Loss: 0.0367\n",
      "Epoch [45/70], Step [300/655], Loss: 0.0696\n",
      "Epoch [45/70], Step [325/655], Loss: 0.0235\n",
      "Epoch [45/70], Step [350/655], Loss: 0.0314\n",
      "Epoch [45/70], Step [375/655], Loss: 0.0418\n",
      "Epoch [45/70], Step [400/655], Loss: 0.0175\n",
      "Epoch [45/70], Step [425/655], Loss: 0.0341\n",
      "Epoch [45/70], Step [450/655], Loss: 0.0212\n",
      "Epoch [45/70], Step [475/655], Loss: 0.0226\n",
      "Epoch [45/70], Step [500/655], Loss: 0.0489\n",
      "Epoch [45/70], Step [525/655], Loss: 0.0207\n",
      "Epoch [45/70], Step [550/655], Loss: 0.0270\n",
      "Epoch [45/70], Step [575/655], Loss: 0.0534\n",
      "Epoch [45/70], Step [600/655], Loss: 0.0265\n",
      "Epoch [45/70], Step [625/655], Loss: 0.0422\n",
      "Epoch [45/70], Step [650/655], Loss: 0.0342\n",
      "Start validation #45\n",
      "Validation #45  Average Loss: 0.6271, mIoU: 0.3585\n",
      "Epoch [46/70], Step [25/655], Loss: 0.0446\n",
      "Epoch [46/70], Step [50/655], Loss: 0.0538\n",
      "Epoch [46/70], Step [75/655], Loss: 0.0569\n",
      "Epoch [46/70], Step [100/655], Loss: 0.0306\n",
      "Epoch [46/70], Step [125/655], Loss: 0.0307\n",
      "Epoch [46/70], Step [150/655], Loss: 0.0134\n",
      "Epoch [46/70], Step [175/655], Loss: 0.0448\n",
      "Epoch [46/70], Step [200/655], Loss: 0.0222\n",
      "Epoch [46/70], Step [225/655], Loss: 0.0177\n",
      "Epoch [46/70], Step [250/655], Loss: 0.0214\n",
      "Epoch [46/70], Step [275/655], Loss: 0.0573\n",
      "Epoch [46/70], Step [300/655], Loss: 0.0337\n",
      "Epoch [46/70], Step [325/655], Loss: 0.0608\n",
      "Epoch [46/70], Step [350/655], Loss: 0.0239\n",
      "Epoch [46/70], Step [375/655], Loss: 0.0240\n",
      "Epoch [46/70], Step [400/655], Loss: 0.0364\n",
      "Epoch [46/70], Step [425/655], Loss: 0.0243\n",
      "Epoch [46/70], Step [450/655], Loss: 0.0364\n",
      "Epoch [46/70], Step [475/655], Loss: 0.0864\n",
      "Epoch [46/70], Step [500/655], Loss: 0.0449\n",
      "Epoch [46/70], Step [525/655], Loss: 0.0310\n",
      "Epoch [46/70], Step [550/655], Loss: 0.0314\n",
      "Epoch [46/70], Step [575/655], Loss: 0.0277\n",
      "Epoch [46/70], Step [600/655], Loss: 0.0252\n",
      "Epoch [46/70], Step [625/655], Loss: 0.0099\n",
      "Epoch [46/70], Step [650/655], Loss: 0.0349\n",
      "Start validation #46\n",
      "Validation #46  Average Loss: 0.9957, mIoU: 0.1929\n",
      "Epoch [47/70], Step [25/655], Loss: 0.0413\n",
      "Epoch [47/70], Step [50/655], Loss: 0.0503\n",
      "Epoch [47/70], Step [75/655], Loss: 0.0151\n",
      "Epoch [47/70], Step [100/655], Loss: 0.0296\n",
      "Epoch [47/70], Step [125/655], Loss: 0.0268\n",
      "Epoch [47/70], Step [150/655], Loss: 0.0630\n",
      "Epoch [47/70], Step [175/655], Loss: 0.0165\n",
      "Epoch [47/70], Step [200/655], Loss: 0.0349\n",
      "Epoch [47/70], Step [225/655], Loss: 0.0314\n",
      "Epoch [47/70], Step [250/655], Loss: 0.0274\n",
      "Epoch [47/70], Step [275/655], Loss: 0.0852\n",
      "Epoch [47/70], Step [300/655], Loss: 0.0177\n",
      "Epoch [47/70], Step [325/655], Loss: 0.0314\n",
      "Epoch [47/70], Step [350/655], Loss: 0.0198\n",
      "Epoch [47/70], Step [375/655], Loss: 0.0276\n",
      "Epoch [47/70], Step [400/655], Loss: 0.0326\n",
      "Epoch [47/70], Step [425/655], Loss: 0.0563\n",
      "Epoch [47/70], Step [450/655], Loss: 0.0444\n",
      "Epoch [47/70], Step [475/655], Loss: 0.0129\n",
      "Epoch [47/70], Step [500/655], Loss: 0.0288\n",
      "Epoch [47/70], Step [525/655], Loss: 0.0247\n",
      "Epoch [47/70], Step [550/655], Loss: 0.1716\n",
      "Epoch [47/70], Step [575/655], Loss: 0.0183\n",
      "Epoch [47/70], Step [600/655], Loss: 0.0478\n",
      "Epoch [47/70], Step [625/655], Loss: 0.0418\n",
      "Epoch [47/70], Step [650/655], Loss: 0.0425\n",
      "Start validation #47\n",
      "Validation #47  Average Loss: 0.5957, mIoU: 0.3256\n",
      "Epoch [48/70], Step [25/655], Loss: 0.0322\n",
      "Epoch [48/70], Step [50/655], Loss: 0.0194\n",
      "Epoch [48/70], Step [75/655], Loss: 0.0225\n",
      "Epoch [48/70], Step [100/655], Loss: 0.0459\n",
      "Epoch [48/70], Step [125/655], Loss: 0.0115\n",
      "Epoch [48/70], Step [150/655], Loss: 0.0213\n",
      "Epoch [48/70], Step [175/655], Loss: 0.0159\n",
      "Epoch [48/70], Step [200/655], Loss: 0.0504\n",
      "Epoch [48/70], Step [225/655], Loss: 0.0532\n",
      "Epoch [48/70], Step [250/655], Loss: 0.0455\n",
      "Epoch [48/70], Step [275/655], Loss: 0.0446\n",
      "Epoch [48/70], Step [300/655], Loss: 0.0235\n",
      "Epoch [48/70], Step [325/655], Loss: 0.0241\n",
      "Epoch [48/70], Step [350/655], Loss: 0.0284\n",
      "Epoch [48/70], Step [375/655], Loss: 0.0447\n",
      "Epoch [48/70], Step [400/655], Loss: 0.0968\n",
      "Epoch [48/70], Step [425/655], Loss: 0.0360\n",
      "Epoch [48/70], Step [450/655], Loss: 0.0328\n",
      "Epoch [48/70], Step [475/655], Loss: 0.0321\n",
      "Epoch [48/70], Step [500/655], Loss: 0.0271\n",
      "Epoch [48/70], Step [525/655], Loss: 0.0281\n",
      "Epoch [48/70], Step [550/655], Loss: 0.0241\n",
      "Epoch [48/70], Step [575/655], Loss: 0.0369\n",
      "Epoch [48/70], Step [600/655], Loss: 0.0350\n",
      "Epoch [48/70], Step [625/655], Loss: 0.0222\n",
      "Epoch [48/70], Step [650/655], Loss: 0.0216\n",
      "Start validation #48\n",
      "Validation #48  Average Loss: 0.8106, mIoU: 0.2416\n",
      "Epoch [49/70], Step [25/655], Loss: 0.0175\n",
      "Epoch [49/70], Step [50/655], Loss: 0.0815\n",
      "Epoch [49/70], Step [75/655], Loss: 0.0170\n",
      "Epoch [49/70], Step [100/655], Loss: 0.0175\n",
      "Epoch [49/70], Step [125/655], Loss: 0.0263\n",
      "Epoch [49/70], Step [150/655], Loss: 0.0363\n",
      "Epoch [49/70], Step [175/655], Loss: 0.0270\n",
      "Epoch [49/70], Step [200/655], Loss: 0.0424\n",
      "Epoch [49/70], Step [225/655], Loss: 0.0198\n",
      "Epoch [49/70], Step [250/655], Loss: 0.0205\n",
      "Epoch [49/70], Step [275/655], Loss: 0.0382\n",
      "Epoch [49/70], Step [300/655], Loss: 0.0336\n",
      "Epoch [49/70], Step [325/655], Loss: 0.0283\n",
      "Epoch [49/70], Step [350/655], Loss: 0.0369\n",
      "Epoch [49/70], Step [375/655], Loss: 0.0408\n",
      "Epoch [49/70], Step [400/655], Loss: 0.0215\n",
      "Epoch [49/70], Step [425/655], Loss: 0.0247\n",
      "Epoch [49/70], Step [450/655], Loss: 0.0395\n",
      "Epoch [49/70], Step [475/655], Loss: 0.0216\n",
      "Epoch [49/70], Step [500/655], Loss: 0.0152\n",
      "Epoch [49/70], Step [525/655], Loss: 0.0447\n",
      "Epoch [49/70], Step [550/655], Loss: 0.0509\n",
      "Epoch [49/70], Step [575/655], Loss: 0.0309\n",
      "Epoch [49/70], Step [600/655], Loss: 0.0344\n",
      "Epoch [49/70], Step [625/655], Loss: 0.0207\n",
      "Epoch [49/70], Step [650/655], Loss: 0.0371\n",
      "Start validation #49\n",
      "Validation #49  Average Loss: 0.8352, mIoU: 0.2345\n",
      "Epoch [50/70], Step [25/655], Loss: 0.0642\n",
      "Epoch [50/70], Step [50/655], Loss: 0.0434\n",
      "Epoch [50/70], Step [75/655], Loss: 0.0182\n",
      "Epoch [50/70], Step [100/655], Loss: 0.0417\n",
      "Epoch [50/70], Step [125/655], Loss: 0.0241\n",
      "Epoch [50/70], Step [150/655], Loss: 0.0203\n",
      "Epoch [50/70], Step [175/655], Loss: 0.0210\n",
      "Epoch [50/70], Step [200/655], Loss: 0.0288\n",
      "Epoch [50/70], Step [225/655], Loss: 0.0334\n",
      "Epoch [50/70], Step [250/655], Loss: 0.0153\n",
      "Epoch [50/70], Step [275/655], Loss: 0.0194\n",
      "Epoch [50/70], Step [300/655], Loss: 0.0282\n",
      "Epoch [50/70], Step [325/655], Loss: 0.0284\n",
      "Epoch [50/70], Step [350/655], Loss: 0.0611\n",
      "Epoch [50/70], Step [375/655], Loss: 0.0468\n",
      "Epoch [50/70], Step [400/655], Loss: 0.0479\n",
      "Epoch [50/70], Step [425/655], Loss: 0.0230\n",
      "Epoch [50/70], Step [450/655], Loss: 0.0269\n",
      "Epoch [50/70], Step [475/655], Loss: 0.0265\n",
      "Epoch [50/70], Step [500/655], Loss: 0.0424\n",
      "Epoch [50/70], Step [525/655], Loss: 0.0235\n",
      "Epoch [50/70], Step [550/655], Loss: 0.0376\n",
      "Epoch [50/70], Step [575/655], Loss: 0.0524\n",
      "Epoch [50/70], Step [600/655], Loss: 0.0287\n",
      "Epoch [50/70], Step [625/655], Loss: 0.0172\n",
      "Epoch [50/70], Step [650/655], Loss: 0.0238\n",
      "Start validation #50\n",
      "Validation #50  Average Loss: 0.7095, mIoU: 0.2616\n",
      "Epoch [51/70], Step [25/655], Loss: 0.0158\n",
      "Epoch [51/70], Step [50/655], Loss: 0.0290\n",
      "Epoch [51/70], Step [75/655], Loss: 0.0188\n",
      "Epoch [51/70], Step [100/655], Loss: 0.0266\n",
      "Epoch [51/70], Step [125/655], Loss: 0.0477\n",
      "Epoch [51/70], Step [150/655], Loss: 0.0375\n",
      "Epoch [51/70], Step [175/655], Loss: 0.0486\n",
      "Epoch [51/70], Step [200/655], Loss: 0.0267\n",
      "Epoch [51/70], Step [225/655], Loss: 0.0318\n",
      "Epoch [51/70], Step [250/655], Loss: 0.0162\n",
      "Epoch [51/70], Step [275/655], Loss: 0.0144\n",
      "Epoch [51/70], Step [300/655], Loss: 0.0212\n",
      "Epoch [51/70], Step [325/655], Loss: 0.0371\n",
      "Epoch [51/70], Step [350/655], Loss: 0.0237\n",
      "Epoch [51/70], Step [375/655], Loss: 0.0304\n",
      "Epoch [51/70], Step [400/655], Loss: 0.0498\n",
      "Epoch [51/70], Step [425/655], Loss: 0.0412\n",
      "Epoch [51/70], Step [450/655], Loss: 0.0208\n",
      "Epoch [51/70], Step [475/655], Loss: 0.0287\n",
      "Epoch [51/70], Step [500/655], Loss: 0.0396\n",
      "Epoch [51/70], Step [525/655], Loss: 0.0265\n",
      "Epoch [51/70], Step [550/655], Loss: 0.0627\n",
      "Epoch [51/70], Step [575/655], Loss: 0.0208\n",
      "Epoch [51/70], Step [600/655], Loss: 0.0236\n",
      "Epoch [51/70], Step [625/655], Loss: 0.0321\n",
      "Epoch [51/70], Step [650/655], Loss: 0.0229\n",
      "Start validation #51\n",
      "Validation #51  Average Loss: 1.1827, mIoU: 0.1642\n",
      "Epoch [52/70], Step [25/655], Loss: 0.0367\n",
      "Epoch [52/70], Step [50/655], Loss: 0.0190\n",
      "Epoch [52/70], Step [75/655], Loss: 0.0432\n",
      "Epoch [52/70], Step [100/655], Loss: 0.0196\n",
      "Epoch [52/70], Step [125/655], Loss: 0.0384\n",
      "Epoch [52/70], Step [150/655], Loss: 0.0469\n",
      "Epoch [52/70], Step [175/655], Loss: 0.0332\n",
      "Epoch [52/70], Step [200/655], Loss: 0.0090\n",
      "Epoch [52/70], Step [225/655], Loss: 0.0672\n",
      "Epoch [52/70], Step [250/655], Loss: 0.0149\n",
      "Epoch [52/70], Step [275/655], Loss: 0.0432\n",
      "Epoch [52/70], Step [300/655], Loss: 0.0309\n",
      "Epoch [52/70], Step [325/655], Loss: 0.0872\n",
      "Epoch [52/70], Step [350/655], Loss: 0.0146\n",
      "Epoch [52/70], Step [375/655], Loss: 0.0119\n",
      "Epoch [52/70], Step [400/655], Loss: 0.0346\n",
      "Epoch [52/70], Step [425/655], Loss: 0.0167\n",
      "Epoch [52/70], Step [450/655], Loss: 0.0420\n",
      "Epoch [52/70], Step [475/655], Loss: 0.0211\n",
      "Epoch [52/70], Step [500/655], Loss: 0.0785\n",
      "Epoch [52/70], Step [525/655], Loss: 0.0330\n",
      "Epoch [52/70], Step [550/655], Loss: 0.0452\n",
      "Epoch [52/70], Step [575/655], Loss: 0.0323\n",
      "Epoch [52/70], Step [600/655], Loss: 0.0154\n",
      "Epoch [52/70], Step [625/655], Loss: 0.0549\n",
      "Epoch [52/70], Step [650/655], Loss: 0.0123\n",
      "Start validation #52\n",
      "Validation #52  Average Loss: 0.7297, mIoU: 0.2512\n",
      "Epoch [53/70], Step [25/655], Loss: 0.0453\n",
      "Epoch [53/70], Step [50/655], Loss: 0.0465\n",
      "Epoch [53/70], Step [75/655], Loss: 0.0117\n",
      "Epoch [53/70], Step [100/655], Loss: 0.0546\n",
      "Epoch [53/70], Step [125/655], Loss: 0.0231\n",
      "Epoch [53/70], Step [150/655], Loss: 0.0232\n",
      "Epoch [53/70], Step [175/655], Loss: 0.0221\n",
      "Epoch [53/70], Step [200/655], Loss: 0.0255\n",
      "Epoch [53/70], Step [225/655], Loss: 0.0309\n",
      "Epoch [53/70], Step [250/655], Loss: 0.0266\n",
      "Epoch [53/70], Step [275/655], Loss: 0.0597\n",
      "Epoch [53/70], Step [300/655], Loss: 0.0174\n",
      "Epoch [53/70], Step [325/655], Loss: 0.0660\n",
      "Epoch [53/70], Step [350/655], Loss: 0.0152\n",
      "Epoch [53/70], Step [375/655], Loss: 0.0210\n",
      "Epoch [53/70], Step [400/655], Loss: 0.0946\n",
      "Epoch [53/70], Step [425/655], Loss: 0.0288\n",
      "Epoch [53/70], Step [450/655], Loss: 0.0321\n",
      "Epoch [53/70], Step [475/655], Loss: 0.0871\n",
      "Epoch [53/70], Step [500/655], Loss: 0.0162\n",
      "Epoch [53/70], Step [525/655], Loss: 0.2189\n",
      "Epoch [53/70], Step [550/655], Loss: 0.0290\n",
      "Epoch [53/70], Step [575/655], Loss: 0.0182\n",
      "Epoch [53/70], Step [600/655], Loss: 0.0264\n",
      "Epoch [53/70], Step [625/655], Loss: 0.0278\n",
      "Epoch [53/70], Step [650/655], Loss: 0.0124\n",
      "Start validation #53\n",
      "Validation #53  Average Loss: 0.7489, mIoU: 0.3170\n",
      "Epoch [54/70], Step [25/655], Loss: 0.0472\n",
      "Epoch [54/70], Step [50/655], Loss: 0.0158\n",
      "Epoch [54/70], Step [75/655], Loss: 0.0193\n",
      "Epoch [54/70], Step [100/655], Loss: 0.0233\n",
      "Epoch [54/70], Step [125/655], Loss: 0.0250\n",
      "Epoch [54/70], Step [150/655], Loss: 0.0241\n",
      "Epoch [54/70], Step [175/655], Loss: 0.0187\n",
      "Epoch [54/70], Step [200/655], Loss: 0.0214\n",
      "Epoch [54/70], Step [225/655], Loss: 0.0361\n",
      "Epoch [54/70], Step [250/655], Loss: 0.0307\n",
      "Epoch [54/70], Step [275/655], Loss: 0.0488\n",
      "Epoch [54/70], Step [300/655], Loss: 0.0164\n",
      "Epoch [54/70], Step [325/655], Loss: 0.0253\n",
      "Epoch [54/70], Step [350/655], Loss: 0.0464\n",
      "Epoch [54/70], Step [375/655], Loss: 0.0288\n",
      "Epoch [54/70], Step [400/655], Loss: 0.0202\n",
      "Epoch [54/70], Step [425/655], Loss: 0.0978\n",
      "Epoch [54/70], Step [450/655], Loss: 0.0348\n",
      "Epoch [54/70], Step [475/655], Loss: 0.0187\n",
      "Epoch [54/70], Step [500/655], Loss: 0.0157\n",
      "Epoch [54/70], Step [525/655], Loss: 0.0579\n",
      "Epoch [54/70], Step [550/655], Loss: 0.0289\n",
      "Epoch [54/70], Step [575/655], Loss: 0.0148\n",
      "Epoch [54/70], Step [600/655], Loss: 0.0121\n",
      "Epoch [54/70], Step [625/655], Loss: 0.0359\n",
      "Epoch [54/70], Step [650/655], Loss: 0.0286\n",
      "Start validation #54\n",
      "Validation #54  Average Loss: 0.4317, mIoU: 0.4566\n",
      "Best performance at epoch: 54\n",
      "Save model in ./saved\n",
      "Epoch [55/70], Step [25/655], Loss: 0.0298\n",
      "Epoch [55/70], Step [50/655], Loss: 0.0259\n",
      "Epoch [55/70], Step [75/655], Loss: 0.0293\n",
      "Epoch [55/70], Step [100/655], Loss: 0.0599\n",
      "Epoch [55/70], Step [125/655], Loss: 0.0224\n",
      "Epoch [55/70], Step [150/655], Loss: 0.0209\n",
      "Epoch [55/70], Step [175/655], Loss: 0.0355\n",
      "Epoch [55/70], Step [200/655], Loss: 0.0164\n",
      "Epoch [55/70], Step [225/655], Loss: 0.0334\n",
      "Epoch [55/70], Step [250/655], Loss: 0.0527\n",
      "Epoch [55/70], Step [275/655], Loss: 0.0721\n",
      "Epoch [55/70], Step [300/655], Loss: 0.0507\n",
      "Epoch [55/70], Step [325/655], Loss: 0.0131\n",
      "Epoch [55/70], Step [350/655], Loss: 0.0219\n",
      "Epoch [55/70], Step [375/655], Loss: 0.0282\n",
      "Epoch [55/70], Step [400/655], Loss: 0.0259\n",
      "Epoch [55/70], Step [425/655], Loss: 0.0127\n",
      "Epoch [55/70], Step [450/655], Loss: 0.0145\n",
      "Epoch [55/70], Step [475/655], Loss: 0.0217\n",
      "Epoch [55/70], Step [500/655], Loss: 0.0565\n",
      "Epoch [55/70], Step [525/655], Loss: 0.0436\n",
      "Epoch [55/70], Step [550/655], Loss: 0.0305\n",
      "Epoch [55/70], Step [575/655], Loss: 0.0232\n",
      "Epoch [55/70], Step [600/655], Loss: 0.0201\n",
      "Epoch [55/70], Step [625/655], Loss: 0.0279\n",
      "Epoch [55/70], Step [650/655], Loss: 0.0179\n",
      "Start validation #55\n",
      "Validation #55  Average Loss: 0.7673, mIoU: 0.2468\n",
      "Epoch [56/70], Step [25/655], Loss: 0.0248\n",
      "Epoch [56/70], Step [50/655], Loss: 0.0418\n",
      "Epoch [56/70], Step [75/655], Loss: 0.0190\n",
      "Epoch [56/70], Step [100/655], Loss: 0.0188\n",
      "Epoch [56/70], Step [125/655], Loss: 0.0264\n",
      "Epoch [56/70], Step [150/655], Loss: 0.0593\n",
      "Epoch [56/70], Step [175/655], Loss: 0.0206\n",
      "Epoch [56/70], Step [200/655], Loss: 0.0360\n",
      "Epoch [56/70], Step [225/655], Loss: 0.0163\n",
      "Epoch [56/70], Step [250/655], Loss: 0.0640\n",
      "Epoch [56/70], Step [275/655], Loss: 0.0464\n",
      "Epoch [56/70], Step [300/655], Loss: 0.0290\n",
      "Epoch [56/70], Step [325/655], Loss: 0.0550\n",
      "Epoch [56/70], Step [350/655], Loss: 0.0140\n",
      "Epoch [56/70], Step [375/655], Loss: 0.0519\n",
      "Epoch [56/70], Step [400/655], Loss: 0.0327\n",
      "Epoch [56/70], Step [425/655], Loss: 0.0438\n",
      "Epoch [56/70], Step [450/655], Loss: 0.0278\n",
      "Epoch [56/70], Step [475/655], Loss: 0.0247\n",
      "Epoch [56/70], Step [500/655], Loss: 0.0243\n",
      "Epoch [56/70], Step [525/655], Loss: 0.0170\n",
      "Epoch [56/70], Step [550/655], Loss: 0.0370\n",
      "Epoch [56/70], Step [575/655], Loss: 0.0187\n",
      "Epoch [56/70], Step [600/655], Loss: 0.0521\n",
      "Epoch [56/70], Step [625/655], Loss: 0.0148\n",
      "Epoch [56/70], Step [650/655], Loss: 0.0340\n",
      "Start validation #56\n",
      "Validation #56  Average Loss: 0.8019, mIoU: 0.3033\n",
      "Epoch [57/70], Step [25/655], Loss: 0.0402\n",
      "Epoch [57/70], Step [50/655], Loss: 0.0422\n",
      "Epoch [57/70], Step [75/655], Loss: 0.0291\n",
      "Epoch [57/70], Step [100/655], Loss: 0.0341\n",
      "Epoch [57/70], Step [125/655], Loss: 0.0206\n",
      "Epoch [57/70], Step [150/655], Loss: 0.0989\n",
      "Epoch [57/70], Step [175/655], Loss: 0.0208\n",
      "Epoch [57/70], Step [200/655], Loss: 0.0237\n",
      "Epoch [57/70], Step [225/655], Loss: 0.0185\n",
      "Epoch [57/70], Step [250/655], Loss: 0.0395\n",
      "Epoch [57/70], Step [275/655], Loss: 0.0210\n",
      "Epoch [57/70], Step [300/655], Loss: 0.0170\n",
      "Epoch [57/70], Step [325/655], Loss: 0.0177\n",
      "Epoch [57/70], Step [350/655], Loss: 0.0410\n",
      "Epoch [57/70], Step [375/655], Loss: 0.0519\n",
      "Epoch [57/70], Step [400/655], Loss: 0.0301\n",
      "Epoch [57/70], Step [425/655], Loss: 0.0182\n",
      "Epoch [57/70], Step [450/655], Loss: 0.0251\n",
      "Epoch [57/70], Step [475/655], Loss: 0.0137\n",
      "Epoch [57/70], Step [500/655], Loss: 0.0214\n",
      "Epoch [57/70], Step [525/655], Loss: 0.0177\n",
      "Epoch [57/70], Step [550/655], Loss: 0.0236\n",
      "Epoch [57/70], Step [575/655], Loss: 0.0535\n",
      "Epoch [57/70], Step [600/655], Loss: 0.0210\n",
      "Epoch [57/70], Step [625/655], Loss: 0.0362\n",
      "Epoch [57/70], Step [650/655], Loss: 0.0236\n",
      "Start validation #57\n",
      "Validation #57  Average Loss: 0.5302, mIoU: 0.4276\n",
      "Epoch [58/70], Step [25/655], Loss: 0.0334\n",
      "Epoch [58/70], Step [50/655], Loss: 0.0213\n",
      "Epoch [58/70], Step [75/655], Loss: 0.0122\n",
      "Epoch [58/70], Step [100/655], Loss: 0.0262\n",
      "Epoch [58/70], Step [125/655], Loss: 0.0292\n",
      "Epoch [58/70], Step [150/655], Loss: 0.0155\n",
      "Epoch [58/70], Step [175/655], Loss: 0.0178\n",
      "Epoch [58/70], Step [200/655], Loss: 0.0435\n",
      "Epoch [58/70], Step [225/655], Loss: 0.0366\n",
      "Epoch [58/70], Step [250/655], Loss: 0.0249\n",
      "Epoch [58/70], Step [275/655], Loss: 0.0200\n",
      "Epoch [58/70], Step [300/655], Loss: 0.0256\n",
      "Epoch [58/70], Step [325/655], Loss: 0.0788\n",
      "Epoch [58/70], Step [350/655], Loss: 0.0129\n",
      "Epoch [58/70], Step [375/655], Loss: 0.0236\n",
      "Epoch [58/70], Step [400/655], Loss: 0.0263\n",
      "Epoch [58/70], Step [425/655], Loss: 0.0413\n",
      "Epoch [58/70], Step [450/655], Loss: 0.0169\n",
      "Epoch [58/70], Step [475/655], Loss: 0.0408\n",
      "Epoch [58/70], Step [500/655], Loss: 0.0341\n",
      "Epoch [58/70], Step [525/655], Loss: 0.0252\n",
      "Epoch [58/70], Step [550/655], Loss: 0.0301\n",
      "Epoch [58/70], Step [575/655], Loss: 0.0272\n",
      "Epoch [58/70], Step [600/655], Loss: 0.0246\n",
      "Epoch [58/70], Step [625/655], Loss: 0.0421\n",
      "Epoch [58/70], Step [650/655], Loss: 0.0612\n",
      "Start validation #58\n",
      "Validation #58  Average Loss: 0.7208, mIoU: 0.3375\n",
      "Epoch [59/70], Step [25/655], Loss: 0.0234\n",
      "Epoch [59/70], Step [50/655], Loss: 0.0308\n",
      "Epoch [59/70], Step [75/655], Loss: 0.0298\n",
      "Epoch [59/70], Step [100/655], Loss: 0.0202\n",
      "Epoch [59/70], Step [125/655], Loss: 0.0431\n",
      "Epoch [59/70], Step [150/655], Loss: 0.0146\n",
      "Epoch [59/70], Step [175/655], Loss: 0.0217\n",
      "Epoch [59/70], Step [200/655], Loss: 0.0224\n",
      "Epoch [59/70], Step [225/655], Loss: 0.0621\n",
      "Epoch [59/70], Step [250/655], Loss: 0.0267\n",
      "Epoch [59/70], Step [275/655], Loss: 0.0143\n",
      "Epoch [59/70], Step [300/655], Loss: 0.0226\n",
      "Epoch [59/70], Step [325/655], Loss: 0.0232\n",
      "Epoch [59/70], Step [350/655], Loss: 0.0320\n",
      "Epoch [59/70], Step [375/655], Loss: 0.0207\n",
      "Epoch [59/70], Step [400/655], Loss: 0.0155\n",
      "Epoch [59/70], Step [425/655], Loss: 0.0516\n",
      "Epoch [59/70], Step [450/655], Loss: 0.0279\n",
      "Epoch [59/70], Step [475/655], Loss: 0.0449\n",
      "Epoch [59/70], Step [500/655], Loss: 0.0131\n",
      "Epoch [59/70], Step [525/655], Loss: 0.0328\n",
      "Epoch [59/70], Step [550/655], Loss: 0.0159\n",
      "Epoch [59/70], Step [575/655], Loss: 0.0394\n",
      "Epoch [59/70], Step [600/655], Loss: 0.0141\n",
      "Epoch [59/70], Step [625/655], Loss: 0.0225\n",
      "Epoch [59/70], Step [650/655], Loss: 0.0388\n",
      "Start validation #59\n",
      "Validation #59  Average Loss: 0.9377, mIoU: 0.2528\n",
      "Epoch [60/70], Step [25/655], Loss: 0.0229\n",
      "Epoch [60/70], Step [50/655], Loss: 0.0264\n",
      "Epoch [60/70], Step [75/655], Loss: 0.0356\n",
      "Epoch [60/70], Step [100/655], Loss: 0.0257\n",
      "Epoch [60/70], Step [125/655], Loss: 0.0166\n",
      "Epoch [60/70], Step [150/655], Loss: 0.0254\n",
      "Epoch [60/70], Step [175/655], Loss: 0.0152\n",
      "Epoch [60/70], Step [200/655], Loss: 0.0294\n",
      "Epoch [60/70], Step [225/655], Loss: 0.0195\n",
      "Epoch [60/70], Step [250/655], Loss: 0.0343\n",
      "Epoch [60/70], Step [275/655], Loss: 0.0202\n",
      "Epoch [60/70], Step [300/655], Loss: 0.0265\n",
      "Epoch [60/70], Step [325/655], Loss: 0.0198\n",
      "Epoch [60/70], Step [350/655], Loss: 0.0183\n",
      "Epoch [60/70], Step [375/655], Loss: 0.0255\n",
      "Epoch [60/70], Step [400/655], Loss: 0.0342\n",
      "Epoch [60/70], Step [425/655], Loss: 0.0212\n",
      "Epoch [60/70], Step [450/655], Loss: 0.0152\n",
      "Epoch [60/70], Step [475/655], Loss: 0.0149\n",
      "Epoch [60/70], Step [500/655], Loss: 0.0639\n",
      "Epoch [60/70], Step [525/655], Loss: 0.0122\n",
      "Epoch [60/70], Step [550/655], Loss: 0.0223\n",
      "Epoch [60/70], Step [575/655], Loss: 0.0445\n",
      "Epoch [60/70], Step [600/655], Loss: 0.0198\n",
      "Epoch [60/70], Step [625/655], Loss: 0.0362\n",
      "Epoch [60/70], Step [650/655], Loss: 0.0276\n",
      "Start validation #60\n",
      "Validation #60  Average Loss: 0.8266, mIoU: 0.2523\n",
      "Epoch [61/70], Step [25/655], Loss: 0.0343\n",
      "Epoch [61/70], Step [50/655], Loss: 0.0455\n",
      "Epoch [61/70], Step [75/655], Loss: 0.0229\n",
      "Epoch [61/70], Step [100/655], Loss: 0.0295\n",
      "Epoch [61/70], Step [125/655], Loss: 0.0278\n",
      "Epoch [61/70], Step [150/655], Loss: 0.0209\n",
      "Epoch [61/70], Step [175/655], Loss: 0.0299\n",
      "Epoch [61/70], Step [200/655], Loss: 0.0227\n",
      "Epoch [61/70], Step [225/655], Loss: 0.0178\n",
      "Epoch [61/70], Step [250/655], Loss: 0.0344\n",
      "Epoch [61/70], Step [275/655], Loss: 0.0455\n",
      "Epoch [61/70], Step [300/655], Loss: 0.0253\n",
      "Epoch [61/70], Step [325/655], Loss: 0.0202\n",
      "Epoch [61/70], Step [350/655], Loss: 0.0241\n",
      "Epoch [61/70], Step [375/655], Loss: 0.0238\n",
      "Epoch [61/70], Step [400/655], Loss: 0.0068\n",
      "Epoch [61/70], Step [425/655], Loss: 0.0143\n",
      "Epoch [61/70], Step [450/655], Loss: 0.0103\n",
      "Epoch [61/70], Step [475/655], Loss: 0.0155\n",
      "Epoch [61/70], Step [500/655], Loss: 0.0215\n",
      "Epoch [61/70], Step [525/655], Loss: 0.0369\n",
      "Epoch [61/70], Step [550/655], Loss: 0.0353\n",
      "Epoch [61/70], Step [575/655], Loss: 0.0191\n",
      "Epoch [61/70], Step [600/655], Loss: 0.0176\n",
      "Epoch [61/70], Step [625/655], Loss: 0.0241\n",
      "Epoch [61/70], Step [650/655], Loss: 0.0198\n",
      "Start validation #61\n",
      "Validation #61  Average Loss: 0.5562, mIoU: 0.3228\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'save_model'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-ff082457d6d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaved_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_every\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-ede35c3daaec>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_epochs, model, data_loader, val_loader, criterion, optimizer, saved_dir, val_every, device)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaved_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./saved/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'save_model'"
     ]
    }
   ],
   "source": [
    "train(CFG.num_epochs, model, train_loader, val_loader, criterion, optimizer, saved_dir, val_every, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 저장된 model 불러오기 (학습된 이후) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T19:44:21.050200Z",
     "start_time": "2021-04-16T19:44:20.802200Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "EMANet(\n",
       "  (extractor): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (1): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (6): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (7): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (8): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (9): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (10): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (11): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (12): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (13): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (14): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (15): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (16): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (17): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (18): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (19): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (20): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (21): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (22): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(2048, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): SynchronizedBatchNorm2d(2048, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(8, 8), dilation=(8, 8), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(2048, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(16, 16), dilation=(16, 16), bias=False)\n",
       "        (bn2): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): SynchronizedBatchNorm2d(2048, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc0): ConvBNReLU(\n",
       "    (conv): Conv2d(2048, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (emau): EMAU(\n",
       "    (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv2): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (fc1): Sequential(\n",
       "    (0): ConvBNReLU(\n",
       "      (conv): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.0003, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Dropout2d(p=0.1, inplace=False)\n",
       "  )\n",
       "  (fc2): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (crit): CrossEntropyLoss2d(\n",
       "    (nll_loss): NLLLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "# best model 저장된 경로\n",
    "model_path = f'./saved/{CFG.version}.pt'\n",
    "\n",
    "# best model 불러오기\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "# 추론을 실행하기 전에는 반드시 설정 (batch normalization, dropout 를 평가 모드로 설정)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T19:44:24.939227Z",
     "start_time": "2021-04-16T19:44:24.518228Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Shape of Original Image : [3, 512, 512]\n",
      "Shape of Predicted :  [512, 512]\n",
      "Unique values, category of transformed mask : \n",
      " [{0, 'Backgroud'}, {'Plastic', 7}, {9, 'Plastic bag'}, {11, 'Clothing'}]\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 1152x1152 with 2 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"457.761776pt\" version=\"1.1\" viewBox=\"0 0 943.69625 457.761776\" width=\"943.69625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 457.761776 \nL 943.69625 457.761776 \nL 943.69625 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 37.69625 430.415838 \nL 443.514432 430.415838 \nL 443.514432 24.597656 \nL 37.69625 24.597656 \nz\n\" style=\"fill:#eaeaf2;\"/>\n   </g>\n   <g clip-path=\"url(#p50be601ded)\">\n    <image height=\"406\" id=\"image2f986633d0\" transform=\"scale(1 -1)translate(0 -406)\" width=\"406\" x=\"37.69625\" xlink:href=\"data:image/png;base64,\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\" y=\"-24.415838\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(34.593182 448.274119)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"text_2\">\n      <!-- 100 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(106.855795 448.274119)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"text_3\">\n      <!-- 200 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(186.117159 448.274119)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"text_4\">\n      <!-- 300 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(265.378523 448.274119)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"text_5\">\n      <!-- 400 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(344.639886 448.274119)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"text_6\">\n      <!-- 500 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(423.90125 448.274119)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"text_7\">\n      <!-- 0 -->\n      <g style=\"fill:#262626;\" transform=\"translate(21.1975 29.173104)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"text_8\">\n      <!-- 100 -->\n      <g style=\"fill:#262626;\" transform=\"translate(7.2 108.434467)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"text_9\">\n      <!-- 200 -->\n      <g style=\"fill:#262626;\" transform=\"translate(7.2 187.695831)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"text_10\">\n      <!-- 300 -->\n      <g style=\"fill:#262626;\" transform=\"translate(7.2 266.957195)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"text_11\">\n      <!-- 400 -->\n      <g style=\"fill:#262626;\" transform=\"translate(7.2 346.218558)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"text_12\">\n      <!-- 500 -->\n      <g style=\"fill:#262626;\" transform=\"translate(7.2 425.479922)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 37.69625 430.415838 \nL 37.69625 24.597656 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 443.514432 430.415838 \nL 443.514432 24.597656 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 37.69625 430.415838 \nL 443.514432 430.415838 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 37.69625 24.597656 \nL 443.514432 24.597656 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"text_13\">\n    <!-- Original image : batch_01_vt/0032.jpg -->\n    <defs>\n     <path d=\"M 39.40625 66.21875 \nQ 28.65625 66.21875 22.328125 58.203125 \nQ 16.015625 50.203125 16.015625 36.375 \nQ 16.015625 22.609375 22.328125 14.59375 \nQ 28.65625 6.59375 39.40625 6.59375 \nQ 50.140625 6.59375 56.421875 14.59375 \nQ 62.703125 22.609375 62.703125 36.375 \nQ 62.703125 50.203125 56.421875 58.203125 \nQ 50.140625 66.21875 39.40625 66.21875 \nz\nM 39.40625 74.21875 \nQ 54.734375 74.21875 63.90625 63.9375 \nQ 73.09375 53.65625 73.09375 36.375 \nQ 73.09375 19.140625 63.90625 8.859375 \nQ 54.734375 -1.421875 39.40625 -1.421875 \nQ 24.03125 -1.421875 14.8125 8.828125 \nQ 5.609375 19.09375 5.609375 36.375 \nQ 5.609375 53.65625 14.8125 63.9375 \nQ 24.03125 74.21875 39.40625 74.21875 \nz\n\" id=\"DejaVuSans-79\"/>\n     <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n     <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n     <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n     <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n     <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n     <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n     <path id=\"DejaVuSans-32\"/>\n     <path d=\"M 52 44.1875 \nQ 55.375 50.25 60.0625 53.125 \nQ 64.75 56 71.09375 56 \nQ 79.640625 56 84.28125 50.015625 \nQ 88.921875 44.046875 88.921875 33.015625 \nL 88.921875 0 \nL 79.890625 0 \nL 79.890625 32.71875 \nQ 79.890625 40.578125 77.09375 44.375 \nQ 74.3125 48.1875 68.609375 48.1875 \nQ 61.625 48.1875 57.5625 43.546875 \nQ 53.515625 38.921875 53.515625 30.90625 \nL 53.515625 0 \nL 44.484375 0 \nL 44.484375 32.71875 \nQ 44.484375 40.625 41.703125 44.40625 \nQ 38.921875 48.1875 33.109375 48.1875 \nQ 26.21875 48.1875 22.15625 43.53125 \nQ 18.109375 38.875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.1875 51.21875 25.484375 53.609375 \nQ 29.78125 56 35.6875 56 \nQ 41.65625 56 45.828125 52.96875 \nQ 50 49.953125 52 44.1875 \nz\n\" id=\"DejaVuSans-109\"/>\n     <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n     <path d=\"M 11.71875 12.40625 \nL 22.015625 12.40625 \nL 22.015625 0 \nL 11.71875 0 \nz\nM 11.71875 51.703125 \nL 22.015625 51.703125 \nL 22.015625 39.3125 \nL 11.71875 39.3125 \nz\n\" id=\"DejaVuSans-58\"/>\n     <path d=\"M 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\nM 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nz\n\" id=\"DejaVuSans-98\"/>\n     <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n     <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n     <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n     <path d=\"M 50.984375 -16.609375 \nL 50.984375 -23.578125 \nL -0.984375 -23.578125 \nL -0.984375 -16.609375 \nz\n\" id=\"DejaVuSans-95\"/>\n     <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n     <path d=\"M 25.390625 72.90625 \nL 33.6875 72.90625 \nL 8.296875 -9.28125 \nL 0 -9.28125 \nz\n\" id=\"DejaVuSans-47\"/>\n     <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n     <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 -0.984375 \nQ 18.40625 -11.421875 14.421875 -16.109375 \nQ 10.453125 -20.796875 1.609375 -20.796875 \nL -1.8125 -20.796875 \nL -1.8125 -13.1875 \nL 0.59375 -13.1875 \nQ 5.71875 -13.1875 7.5625 -10.8125 \nQ 9.421875 -8.453125 9.421875 -0.984375 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-106\"/>\n     <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n    </defs>\n    <g style=\"fill:#262626;\" transform=\"translate(97.016669 18.597656)scale(0.15 -0.15)\">\n     <use xlink:href=\"#DejaVuSans-79\"/>\n     <use x=\"78.710938\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"119.824219\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"147.607422\" xlink:href=\"#DejaVuSans-103\"/>\n     <use x=\"211.083984\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"238.867188\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"302.246094\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"363.525391\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"391.308594\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"423.095703\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"450.878906\" xlink:href=\"#DejaVuSans-109\"/>\n     <use x=\"548.291016\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"609.570312\" xlink:href=\"#DejaVuSans-103\"/>\n     <use x=\"673.046875\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"734.570312\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"766.357422\" xlink:href=\"#DejaVuSans-58\"/>\n     <use x=\"800.048828\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"831.835938\" xlink:href=\"#DejaVuSans-98\"/>\n     <use x=\"895.3125\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"956.591797\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"995.800781\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"1050.78125\" xlink:href=\"#DejaVuSans-104\"/>\n     <use x=\"1114.160156\" xlink:href=\"#DejaVuSans-95\"/>\n     <use x=\"1164.160156\" xlink:href=\"#DejaVuSans-48\"/>\n     <use x=\"1227.783203\" xlink:href=\"#DejaVuSans-49\"/>\n     <use x=\"1291.40625\" xlink:href=\"#DejaVuSans-95\"/>\n     <use x=\"1341.40625\" xlink:href=\"#DejaVuSans-118\"/>\n     <use x=\"1400.585938\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"1439.794922\" xlink:href=\"#DejaVuSans-47\"/>\n     <use x=\"1473.486328\" xlink:href=\"#DejaVuSans-48\"/>\n     <use x=\"1537.109375\" xlink:href=\"#DejaVuSans-48\"/>\n     <use x=\"1600.732422\" xlink:href=\"#DejaVuSans-51\"/>\n     <use x=\"1664.355469\" xlink:href=\"#DejaVuSans-50\"/>\n     <use x=\"1727.978516\" xlink:href=\"#DejaVuSans-46\"/>\n     <use x=\"1759.765625\" xlink:href=\"#DejaVuSans-106\"/>\n     <use x=\"1787.548828\" xlink:href=\"#DejaVuSans-112\"/>\n     <use x=\"1851.025391\" xlink:href=\"#DejaVuSans-103\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_7\">\n    <path d=\"M 524.678068 430.415838 \nL 930.49625 430.415838 \nL 930.49625 24.597656 \nL 524.678068 24.597656 \nz\n\" style=\"fill:#eaeaf2;\"/>\n   </g>\n   <g clip-path=\"url(#p46bdc00d74)\">\n    <image height=\"406\" id=\"image3a8fba971e\" transform=\"scale(1 -1)translate(0 -406)\" width=\"406\" x=\"524.678068\" xlink:href=\"data:image/png;base64,\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\" y=\"-24.415838\"/>\n   </g>\n   <g id=\"matplotlib.axis_3\">\n    <g id=\"xtick_7\">\n     <g id=\"text_14\">\n      <!-- 0 -->\n      <g style=\"fill:#262626;\" transform=\"translate(521.575 448.274119)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"text_15\">\n      <!-- 100 -->\n      <g style=\"fill:#262626;\" transform=\"translate(593.837614 448.274119)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"text_16\">\n      <!-- 200 -->\n      <g style=\"fill:#262626;\" transform=\"translate(673.098977 448.274119)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"text_17\">\n      <!-- 300 -->\n      <g style=\"fill:#262626;\" transform=\"translate(752.360341 448.274119)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_11\">\n     <g id=\"text_18\">\n      <!-- 400 -->\n      <g style=\"fill:#262626;\" transform=\"translate(831.621705 448.274119)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_12\">\n     <g id=\"text_19\">\n      <!-- 500 -->\n      <g style=\"fill:#262626;\" transform=\"translate(910.883068 448.274119)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_7\">\n     <g id=\"text_20\">\n      <!-- 0 -->\n      <g style=\"fill:#262626;\" transform=\"translate(508.179318 29.173104)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"text_21\">\n      <!-- 100 -->\n      <g style=\"fill:#262626;\" transform=\"translate(494.181818 108.434467)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"text_22\">\n      <!-- 200 -->\n      <g style=\"fill:#262626;\" transform=\"translate(494.181818 187.695831)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"text_23\">\n      <!-- 300 -->\n      <g style=\"fill:#262626;\" transform=\"translate(494.181818 266.957195)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"text_24\">\n      <!-- 400 -->\n      <g style=\"fill:#262626;\" transform=\"translate(494.181818 346.218558)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"text_25\">\n      <!-- 500 -->\n      <g style=\"fill:#262626;\" transform=\"translate(494.181818 425.479922)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_8\">\n    <path d=\"M 524.678068 430.415838 \nL 524.678068 24.597656 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 930.49625 430.415838 \nL 930.49625 24.597656 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 524.678068 430.415838 \nL 930.49625 430.415838 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path d=\"M 524.678068 24.597656 \nL 930.49625 24.597656 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"text_26\">\n    <!-- Predicted : batch_01_vt/0032.jpg -->\n    <defs>\n     <path d=\"M 19.671875 64.796875 \nL 19.671875 37.40625 \nL 32.078125 37.40625 \nQ 38.96875 37.40625 42.71875 40.96875 \nQ 46.484375 44.53125 46.484375 51.125 \nQ 46.484375 57.671875 42.71875 61.234375 \nQ 38.96875 64.796875 32.078125 64.796875 \nz\nM 9.8125 72.90625 \nL 32.078125 72.90625 \nQ 44.34375 72.90625 50.609375 67.359375 \nQ 56.890625 61.8125 56.890625 51.125 \nQ 56.890625 40.328125 50.609375 34.8125 \nQ 44.34375 29.296875 32.078125 29.296875 \nL 19.671875 29.296875 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-80\"/>\n     <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n    </defs>\n    <g style=\"fill:#262626;\" transform=\"translate(603.589893 18.597656)scale(0.15 -0.15)\">\n     <use xlink:href=\"#DejaVuSans-80\"/>\n     <use x=\"60.287109\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"101.369141\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"162.892578\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"226.369141\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"254.152344\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"309.132812\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"348.341797\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"409.865234\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"473.341797\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"505.128906\" xlink:href=\"#DejaVuSans-58\"/>\n     <use x=\"538.820312\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"570.607422\" xlink:href=\"#DejaVuSans-98\"/>\n     <use x=\"634.083984\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"695.363281\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"734.572266\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"789.552734\" xlink:href=\"#DejaVuSans-104\"/>\n     <use x=\"852.931641\" xlink:href=\"#DejaVuSans-95\"/>\n     <use x=\"902.931641\" xlink:href=\"#DejaVuSans-48\"/>\n     <use x=\"966.554688\" xlink:href=\"#DejaVuSans-49\"/>\n     <use x=\"1030.177734\" xlink:href=\"#DejaVuSans-95\"/>\n     <use x=\"1080.177734\" xlink:href=\"#DejaVuSans-118\"/>\n     <use x=\"1139.357422\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"1178.566406\" xlink:href=\"#DejaVuSans-47\"/>\n     <use x=\"1212.257812\" xlink:href=\"#DejaVuSans-48\"/>\n     <use x=\"1275.880859\" xlink:href=\"#DejaVuSans-48\"/>\n     <use x=\"1339.503906\" xlink:href=\"#DejaVuSans-51\"/>\n     <use x=\"1403.126953\" xlink:href=\"#DejaVuSans-50\"/>\n     <use x=\"1466.75\" xlink:href=\"#DejaVuSans-46\"/>\n     <use x=\"1498.537109\" xlink:href=\"#DejaVuSans-106\"/>\n     <use x=\"1526.320312\" xlink:href=\"#DejaVuSans-112\"/>\n     <use x=\"1589.796875\" xlink:href=\"#DejaVuSans-103\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p50be601ded\">\n   <rect height=\"405.818182\" width=\"405.818182\" x=\"37.69625\" y=\"24.597656\"/>\n  </clipPath>\n  <clipPath id=\"p46bdc00d74\">\n   <rect height=\"405.818182\" width=\"405.818182\" x=\"524.678068\" y=\"24.597656\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "# 첫번째 batch의 추론 결과 확인\n",
    "for imgs, image_infos in test_loader:\n",
    "    image_infos = image_infos\n",
    "    temp_images = imgs\n",
    "    \n",
    "    model.eval()\n",
    "    # inference\n",
    "    outs = model(torch.stack(temp_images).to(device))\n",
    "    oms = torch.argmax(outs.squeeze(), dim=1).detach().cpu().numpy()\n",
    "    \n",
    "    break\n",
    "\n",
    "i = 3\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(16, 16))\n",
    "\n",
    "print('Shape of Original Image :', list(temp_images[i].shape))\n",
    "print('Shape of Predicted : ', list(oms[i].shape))\n",
    "print('Unique values, category of transformed mask : \\n', [{int(i),category_names[int(i)]} for i in list(np.unique(oms[i]))])\n",
    "\n",
    "# Original image\n",
    "ax1.imshow(temp_images[i].permute([1,2,0]))\n",
    "ax1.grid(False)\n",
    "ax1.set_title(\"Original image : {}\".format(image_infos[i]['file_name']), fontsize = 15)\n",
    "\n",
    "# Predicted\n",
    "ax2.imshow(oms[i])\n",
    "ax2.grid(False)\n",
    "ax2.set_title(\"Predicted : {}\".format(image_infos[i]['file_name']), fontsize = 15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## submission을 위한 test 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T19:44:27.469285Z",
     "start_time": "2021-04-16T19:44:27.456021Z"
    }
   },
   "outputs": [],
   "source": [
    "def test(model, data_loader, device):\n",
    "    size = 256\n",
    "    transform = A.Compose([A.Resize(256, 256)])\n",
    "    print('Start prediction.')\n",
    "    model.eval()\n",
    "    \n",
    "    file_name_list = []\n",
    "    preds_array = np.empty((0, size*size), dtype=np.long)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step, (imgs, image_infos) in enumerate(test_loader):\n",
    "\n",
    "            # inference (512 x 512)\n",
    "            outs = model(torch.stack(imgs).to(device))\n",
    "            oms = torch.argmax(outs.squeeze(), dim=1).detach().cpu().numpy()\n",
    "            \n",
    "            # resize (256 x 256)\n",
    "            temp_mask = []\n",
    "            for img, mask in zip(np.stack(temp_images), oms):\n",
    "                transformed = transform(image=img, mask=mask)\n",
    "                mask = transformed['mask']\n",
    "                temp_mask.append(mask)\n",
    "\n",
    "            oms = np.array(temp_mask)\n",
    "            \n",
    "            oms = oms.reshape([oms.shape[0], size*size]).astype(int)\n",
    "            preds_array = np.vstack((preds_array, oms))\n",
    "            \n",
    "            file_name_list.append([i['file_name'] for i in image_infos])\n",
    "    print(\"End prediction.\")\n",
    "    file_names = [y for x in file_name_list for y in x]\n",
    "    \n",
    "    return file_names, preds_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## submission.csv 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T19:45:42.235310Z",
     "start_time": "2021-04-16T19:44:30.499016Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start prediction.\n",
      "End prediction.\n"
     ]
    }
   ],
   "source": [
    "# sample_submisson.csv 열기\n",
    "submission = pd.read_csv('./submission/sample_submission.csv', index_col=None)\n",
    "\n",
    "# test set에 대한 prediction\n",
    "file_names, preds = test(model, test_loader, device)\n",
    "\n",
    "# PredictionString 대입\n",
    "for file_name, string in zip(file_names, preds):\n",
    "    submission = submission.append({\"image_id\" : file_name, \"PredictionString\" : ' '.join(str(e) for e in string.tolist())}, \n",
    "                                   ignore_index=True)\n",
    "\n",
    "# submission.csv로 저장\n",
    "submission.to_csv(f\"./submission/{CFG.version}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyper-param 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "name": "python377jvsc74a57bd098b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f",
   "display_name": "Python 3.7.7 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "297.278px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}