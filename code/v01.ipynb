{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T11:06:58.944902Z",
     "start_time": "2021-04-22T11:06:56.623974Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version: 1.4.0\n",
      "GPU 사용 가능 여부: True\n",
      "Tesla V100-PCIE-32GB\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import warnings \n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from model import *\n",
    "from utils import label_accuracy_score\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 전처리를 위한 라이브러리\n",
    "from pycocotools.coco import COCO\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# 시각화를 위한 라이브러리\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from adamp import AdamP\n",
    "\n",
    "plt.rcParams['axes.grid'] = False\n",
    "\n",
    "print('pytorch version: {}'.format(torch.__version__))\n",
    "print('GPU 사용 가능 여부: {}'.format(torch.cuda.is_available()))\n",
    "\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"   # GPU 사용 가능 여부에 따라 device 정보 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼파라미터 세팅 및 seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T11:06:59.171980Z",
     "start_time": "2021-04-22T11:06:59.167952Z"
    }
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    version = 'v01'\n",
    "    \n",
    "    # 하이퍼 파라미터 설정\n",
    "    batch_size = 16\n",
    "    num_epochs = 20\n",
    "    lr = 0.0001\n",
    "    SEED = 21\n",
    "    num_classes = 12\n",
    "    weight_decay = 1e-6\n",
    "    loss_weight = None\n",
    "    \n",
    "    # Model, Loss, Optimizer 및 하이퍼 파라미터 선택\n",
    "    model = 'FCN8s' # FCN8s , \n",
    "    model_params = {\n",
    "        'FCN8s':{'num_classes':num_classes},\n",
    "    }\n",
    "    loss = 'CE' # CE , \n",
    "    loss_params = {\n",
    "        'CE':{'weight':loss_weight},\n",
    "    }\n",
    "    optim = 'Adam' # Adam , AdamP\n",
    "    optim_params = {\n",
    "        'Adam':{'lr':lr,'weight_decay':weight_decay},\n",
    "        'AdamP':{'lr':lr,'weight_decay':weight_decay},\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T11:06:59.446510Z",
     "start_time": "2021-04-22T11:06:59.443508Z"
    }
   },
   "outputs": [],
   "source": [
    "# seed 고정\n",
    "random_seed = CFG.SEED\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "# torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = {'FCN8s':FCN8s,}\n",
    "loss_list = {'CE':nn.CrossEntropyLoss,}\n",
    "optim_list = {'Adam':optim.Adam, 'AdamP':AdamP}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 데이터 EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T11:07:04.139668Z",
     "start_time": "2021-04-22T11:07:00.575728Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of super categories: 11\n",
      "Number of categories: 11\n",
      "Number of annotations: 21116\n",
      "Number of images: 2617\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "dataset_path = '../input/data'\n",
    "anns_file_path = dataset_path + '/' + 'train.json'\n",
    "\n",
    "# Read annotations\n",
    "with open(anns_file_path, 'r') as f:\n",
    "    dataset = json.loads(f.read())\n",
    "\n",
    "categories = dataset['categories']\n",
    "anns = dataset['annotations']\n",
    "imgs = dataset['images']\n",
    "nr_cats = len(categories)\n",
    "nr_annotations = len(anns)\n",
    "nr_images = len(imgs)\n",
    "\n",
    "# Load categories and super categories\n",
    "cat_names = []\n",
    "super_cat_names = []\n",
    "super_cat_ids = {}\n",
    "super_cat_last_name = ''\n",
    "nr_super_cats = 0\n",
    "for cat_it in categories:\n",
    "    cat_names.append(cat_it['name'])\n",
    "    super_cat_name = cat_it['supercategory']\n",
    "    # Adding new supercat\n",
    "    if super_cat_name != super_cat_last_name:\n",
    "        super_cat_names.append(super_cat_name)\n",
    "        super_cat_ids[super_cat_name] = nr_super_cats\n",
    "        super_cat_last_name = super_cat_name\n",
    "        nr_super_cats += 1\n",
    "\n",
    "print('Number of super categories:', nr_super_cats)\n",
    "print('Number of categories:', nr_cats)\n",
    "print('Number of annotations:', nr_annotations)\n",
    "print('Number of images:', nr_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T11:07:04.394832Z",
     "start_time": "2021-04-22T11:07:04.141668Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Count annotations\n",
    "cat_histogram = np.zeros(nr_cats,dtype=int)\n",
    "for ann in anns:\n",
    "    cat_histogram[ann['category_id']] += 1\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame({'Categories': cat_names, 'Number of annotations': cat_histogram})\n",
    "df = df.sort_values('Number of annotations', 0, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T11:07:04.409808Z",
     "start_time": "2021-04-22T11:07:04.395831Z"
    }
   },
   "outputs": [],
   "source": [
    "# category labeling \n",
    "sorted_temp_df = df.sort_index()\n",
    "\n",
    "# background = 0 에 해당되는 label 추가 후 기존들을 모두 label + 1 로 설정\n",
    "sorted_df = pd.DataFrame([\"Backgroud\"], columns = [\"Categories\"])\n",
    "sorted_df = sorted_df.append(sorted_temp_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리 함수 정의 (Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T11:07:04.439837Z",
     "start_time": "2021-04-22T11:07:04.425804Z"
    }
   },
   "outputs": [],
   "source": [
    "category_names = list(sorted_df.Categories)\n",
    "\n",
    "def get_classname(classID, cats):\n",
    "    for i in range(len(cats)):\n",
    "        if cats[i]['id']==classID:\n",
    "            return cats[i]['name']\n",
    "    return \"None\"\n",
    "\n",
    "class CustomDataLoader(Dataset):\n",
    "    \"\"\"COCO format\"\"\"\n",
    "    def __init__(self, data_dir, mode = 'train', transform = None):\n",
    "        super().__init__()\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.coco = COCO(data_dir)\n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        # dataset이 index되어 list처럼 동작\n",
    "        image_id = self.coco.getImgIds(imgIds=index)\n",
    "        image_infos = self.coco.loadImgs(image_id)[0]\n",
    "        \n",
    "        # cv2 를 활용하여 image 불러오기\n",
    "        images = cv2.imread(os.path.join(dataset_path, image_infos['file_name']))\n",
    "        images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        images /= 255.0\n",
    "        \n",
    "        if (self.mode in ('train', 'val')):\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=image_infos['id'])\n",
    "            anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "            # Load the categories in a variable\n",
    "            cat_ids = self.coco.getCatIds()\n",
    "            cats = self.coco.loadCats(cat_ids)\n",
    "\n",
    "            # masks : size가 (height x width)인 2D\n",
    "            # 각각의 pixel 값에는 \"category id + 1\" 할당\n",
    "            # Background = 0\n",
    "            masks = np.zeros((image_infos[\"height\"], image_infos[\"width\"]))\n",
    "            # Unknown = 1, General trash = 2, ... , Cigarette = 11\n",
    "            for i in range(len(anns)):\n",
    "                className = get_classname(anns[i]['category_id'], cats)\n",
    "                pixel_value = category_names.index(className)\n",
    "                masks = np.maximum(self.coco.annToMask(anns[i])*pixel_value, masks)\n",
    "            masks = masks.astype(np.float32)\n",
    "\n",
    "            # transform -> albumentations 라이브러리 활용\n",
    "            if self.transform is not None:\n",
    "                transformed = self.transform(image=images, mask=masks)\n",
    "                images = transformed[\"image\"]\n",
    "                masks = transformed[\"mask\"]\n",
    "            \n",
    "            return images, masks, image_infos\n",
    "        \n",
    "        if self.mode == 'test':\n",
    "            # transform -> albumentations 라이브러리 활용\n",
    "            if self.transform is not None:\n",
    "                transformed = self.transform(image=images)\n",
    "                images = transformed[\"image\"]\n",
    "            \n",
    "            return images, image_infos\n",
    "    \n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        # 전체 dataset의 size를 return\n",
    "        return len(self.coco.getImgIds())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 정의 및 DataLoader 할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T11:07:09.179806Z",
     "start_time": "2021-04-22T11:07:04.440804Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=4.01s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.81s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# train.json / validation.json / test.json 디렉토리 설정\n",
    "train_path = dataset_path + '/train.json'\n",
    "val_path = dataset_path + '/val.json'\n",
    "test_path = dataset_path + '/test.json'\n",
    "\n",
    "# collate_fn needs for batch\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_transform = A.Compose([\n",
    "                            ToTensorV2()\n",
    "                            ])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "                          ToTensorV2()\n",
    "                          ])\n",
    "\n",
    "test_transform = A.Compose([\n",
    "                           ToTensorV2()\n",
    "                           ])\n",
    "\n",
    "# create own Dataset 1 (skip)\n",
    "# validation set을 직접 나누고 싶은 경우\n",
    "# random_split 사용하여 data set을 8:2 로 분할\n",
    "# train_size = int(0.8*len(dataset))\n",
    "# val_size = int(len(dataset)-train_size)\n",
    "# dataset = CustomDataLoader(data_dir=train_path, mode='train', transform=transform)\n",
    "# train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# create own Dataset 2\n",
    "# train dataset\n",
    "train_dataset = CustomDataLoader(data_dir=train_path, mode='train', transform=train_transform)\n",
    "\n",
    "# validation dataset\n",
    "val_dataset = CustomDataLoader(data_dir=val_path, mode='val', transform=val_transform)\n",
    "\n",
    "# test dataset\n",
    "test_dataset = CustomDataLoader(data_dir=test_path, mode='test', transform=test_transform)\n",
    "\n",
    "\n",
    "# DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=CFG.batch_size,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=4,\n",
    "                                           collate_fn=collate_fn)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                         batch_size=CFG.batch_size,\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=4,\n",
    "                                         collate_fn=collate_fn)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=CFG.batch_size,\n",
    "                                          num_workers=4,\n",
    "                                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T11:15:34.624277Z",
     "start_time": "2021-04-22T11:15:30.068347Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape :  torch.Size([1, 3, 512, 512])\n",
      "output shape :  torch.Size([1, 12, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "# 구현된 model에 임의의 input을 넣어 output이 잘 나오는지 test\n",
    "model_name = CFG.model\n",
    "model = model_list[model_name](**CFG.model_params[model_name])\n",
    "\n",
    "x = torch.randn([1, 3, 512, 512])\n",
    "print(\"input shape : \", x.shape)\n",
    "out = model(x).to(device)\n",
    "print(\"output shape : \", out.size())\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train, validation, test 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T11:15:38.201874Z",
     "start_time": "2021-04-22T11:15:38.187884Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(num_epochs, model, data_loader, val_loader, criterion, optimizer, saved_dir, val_every, device):\n",
    "    print('Start training..')\n",
    "    best_loss = 9999999\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for step, (images, masks, _) in enumerate(data_loader):\n",
    "            images = torch.stack(images)       # (batch, channel, height, width)\n",
    "            masks = torch.stack(masks).long()  # (batch, channel, height, width)\n",
    "            \n",
    "            # gpu 연산을 위해 device 할당\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "                  \n",
    "            # inference\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # loss 계산 (cross entropy loss)\n",
    "            loss = criterion(outputs, masks)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # step 주기에 따른 loss 출력\n",
    "            if (step + 1) % 25 == 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(\n",
    "                    epoch+1, num_epochs, step+1, len(train_loader), loss.item()))\n",
    "        \n",
    "        # validation 주기에 따른 loss 출력 및 best model 저장\n",
    "        if (epoch + 1) % val_every == 0:\n",
    "            avrg_loss = validation(epoch + 1, model, val_loader, criterion, device)\n",
    "            if avrg_loss < best_loss:\n",
    "                print('Best performance at epoch: {}'.format(epoch + 1))\n",
    "                print('Save model in', saved_dir)\n",
    "                best_loss = avrg_loss\n",
    "                save_model(model, saved_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T11:15:38.901226Z",
     "start_time": "2021-04-22T11:15:38.888195Z"
    }
   },
   "outputs": [],
   "source": [
    "def validation(epoch, model, data_loader, criterion, device):\n",
    "    print('Start validation #{}'.format(epoch))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        cnt = 0\n",
    "        mIoU_list = []\n",
    "        for step, (images, masks, _) in enumerate(data_loader):\n",
    "            \n",
    "            images = torch.stack(images)       # (batch, channel, height, width)\n",
    "            masks = torch.stack(masks).long()  # (batch, channel, height, width)\n",
    "\n",
    "            images, masks = images.to(device), masks.to(device)            \n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            total_loss += loss\n",
    "            cnt += 1\n",
    "            \n",
    "            outputs = torch.argmax(outputs.squeeze(), dim=1).detach().cpu().numpy()\n",
    "\n",
    "            mIoU = label_accuracy_score(masks.detach().cpu().numpy(), outputs, n_class=12)[2]\n",
    "            mIoU_list.append(mIoU)\n",
    "            \n",
    "        avrg_loss = total_loss / cnt\n",
    "        print('Validation #{}  Average Loss: {:.4f}, mIoU: {:.4f}'.format(epoch, avrg_loss, np.mean(mIoU_list)))\n",
    "\n",
    "    return avrg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 저장 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T11:15:41.634492Z",
     "start_time": "2021-04-22T11:15:41.627493Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 저장 함수 정의\n",
    "val_every = 1 \n",
    "\n",
    "saved_dir = './saved'\n",
    "if not os.path.isdir(saved_dir):                                                           \n",
    "    os.mkdir(saved_dir)\n",
    "    \n",
    "def save_model(model, saved_dir, file_name=f'{CFG.version}.pt'):\n",
    "    check_point = {'net': model.state_dict()}\n",
    "    output_path = os.path.join(saved_dir, file_name)\n",
    "    torch.save(model.state_dict(), output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 생성 및 Loss function, Optimizer 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T11:15:43.106368Z",
     "start_time": "2021-04-22T11:15:43.096368Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loss function 정의\n",
    "loss_name = CFG.loss\n",
    "criterion = loss_list[loss_name](**CFG.loss_params[loss_name])\n",
    "\n",
    "# Optimizer 정의\n",
    "optim_name = CFG.optim\n",
    "optimizer = optim_list[optim_name](params = model.parameters(), **CFG.optim_params[optim_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-22T11:15:43.700Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training..\n",
      "Epoch [1/20], Step [25/164], Loss: 1.1734\n",
      "Epoch [1/20], Step [50/164], Loss: 0.9999\n",
      "Epoch [1/20], Step [75/164], Loss: 0.8136\n",
      "Epoch [1/20], Step [100/164], Loss: 0.8986\n",
      "Epoch [1/20], Step [125/164], Loss: 0.9223\n",
      "Epoch [1/20], Step [150/164], Loss: 1.0907\n",
      "Start validation #1\n",
      "Validation #1  Average Loss: 0.8923, mIoU: 0.1070\n",
      "Best performance at epoch: 1\n",
      "Save model in ./saved\n",
      "Epoch [2/20], Step [25/164], Loss: 1.2653\n",
      "Epoch [2/20], Step [50/164], Loss: 1.0200\n",
      "Epoch [2/20], Step [75/164], Loss: 0.6469\n",
      "Epoch [2/20], Step [100/164], Loss: 0.5586\n",
      "Epoch [2/20], Step [125/164], Loss: 0.6994\n",
      "Epoch [2/20], Step [150/164], Loss: 0.6920\n",
      "Start validation #2\n",
      "Validation #2  Average Loss: 0.6918, mIoU: 0.1827\n",
      "Best performance at epoch: 2\n",
      "Save model in ./saved\n",
      "Epoch [3/20], Step [25/164], Loss: 0.6852\n",
      "Epoch [3/20], Step [50/164], Loss: 0.5852\n",
      "Epoch [3/20], Step [75/164], Loss: 0.5898\n",
      "Epoch [3/20], Step [100/164], Loss: 0.6457\n",
      "Epoch [3/20], Step [125/164], Loss: 0.5732\n",
      "Epoch [3/20], Step [150/164], Loss: 0.8321\n",
      "Start validation #3\n",
      "Validation #3  Average Loss: 0.6103, mIoU: 0.1943\n",
      "Best performance at epoch: 3\n",
      "Save model in ./saved\n",
      "Epoch [4/20], Step [25/164], Loss: 0.4274\n",
      "Epoch [4/20], Step [50/164], Loss: 0.6656\n",
      "Epoch [4/20], Step [75/164], Loss: 0.5565\n",
      "Epoch [4/20], Step [100/164], Loss: 0.7583\n",
      "Epoch [4/20], Step [125/164], Loss: 0.5270\n",
      "Epoch [4/20], Step [150/164], Loss: 0.4724\n",
      "Start validation #4\n",
      "Validation #4  Average Loss: 0.5816, mIoU: 0.1981\n",
      "Best performance at epoch: 4\n",
      "Save model in ./saved\n",
      "Epoch [5/20], Step [25/164], Loss: 0.5159\n",
      "Epoch [5/20], Step [50/164], Loss: 0.8047\n",
      "Epoch [5/20], Step [75/164], Loss: 0.4736\n",
      "Epoch [5/20], Step [100/164], Loss: 0.5785\n",
      "Epoch [5/20], Step [125/164], Loss: 0.4722\n",
      "Epoch [5/20], Step [150/164], Loss: 0.6080\n",
      "Start validation #5\n",
      "Validation #5  Average Loss: 0.5352, mIoU: 0.2021\n",
      "Best performance at epoch: 5\n",
      "Save model in ./saved\n",
      "Epoch [6/20], Step [25/164], Loss: 0.6547\n",
      "Epoch [6/20], Step [50/164], Loss: 0.5857\n",
      "Epoch [6/20], Step [75/164], Loss: 0.5637\n",
      "Epoch [6/20], Step [100/164], Loss: 0.3774\n",
      "Epoch [6/20], Step [125/164], Loss: 0.4055\n",
      "Epoch [6/20], Step [150/164], Loss: 0.4281\n",
      "Start validation #6\n",
      "Validation #6  Average Loss: 0.5344, mIoU: 0.1868\n",
      "Best performance at epoch: 6\n",
      "Save model in ./saved\n",
      "Epoch [7/20], Step [25/164], Loss: 0.5071\n",
      "Epoch [7/20], Step [50/164], Loss: 0.5562\n",
      "Epoch [7/20], Step [75/164], Loss: 0.7259\n",
      "Epoch [7/20], Step [100/164], Loss: 0.6907\n",
      "Epoch [7/20], Step [125/164], Loss: 0.4887\n",
      "Epoch [7/20], Step [150/164], Loss: 0.4428\n",
      "Start validation #7\n",
      "Validation #7  Average Loss: 0.4899, mIoU: 0.1934\n",
      "Best performance at epoch: 7\n",
      "Save model in ./saved\n",
      "Epoch [8/20], Step [25/164], Loss: 0.3962\n",
      "Epoch [8/20], Step [50/164], Loss: 0.3308\n",
      "Epoch [8/20], Step [75/164], Loss: 0.4034\n",
      "Epoch [8/20], Step [100/164], Loss: 0.5640\n",
      "Epoch [8/20], Step [125/164], Loss: 0.5370\n",
      "Epoch [8/20], Step [150/164], Loss: 0.5381\n",
      "Start validation #8\n",
      "Validation #8  Average Loss: 0.5118, mIoU: 0.1812\n",
      "Epoch [9/20], Step [25/164], Loss: 0.2150\n",
      "Epoch [9/20], Step [50/164], Loss: 0.5605\n",
      "Epoch [9/20], Step [75/164], Loss: 0.4444\n",
      "Epoch [9/20], Step [100/164], Loss: 0.4826\n",
      "Epoch [9/20], Step [125/164], Loss: 0.4169\n",
      "Epoch [9/20], Step [150/164], Loss: 0.6705\n",
      "Start validation #9\n",
      "Validation #9  Average Loss: 0.4900, mIoU: 0.2140\n",
      "Epoch [10/20], Step [25/164], Loss: 0.3259\n",
      "Epoch [10/20], Step [50/164], Loss: 0.4005\n",
      "Epoch [10/20], Step [75/164], Loss: 0.4309\n",
      "Epoch [10/20], Step [100/164], Loss: 0.4285\n",
      "Epoch [10/20], Step [125/164], Loss: 0.6097\n",
      "Epoch [10/20], Step [150/164], Loss: 0.3654\n",
      "Start validation #10\n",
      "Validation #10  Average Loss: 0.4758, mIoU: 0.2267\n",
      "Best performance at epoch: 10\n",
      "Save model in ./saved\n",
      "Epoch [11/20], Step [25/164], Loss: 0.3059\n",
      "Epoch [11/20], Step [50/164], Loss: 0.5801\n",
      "Epoch [11/20], Step [75/164], Loss: 0.3237\n",
      "Epoch [11/20], Step [100/164], Loss: 0.5620\n",
      "Epoch [11/20], Step [125/164], Loss: 0.3883\n",
      "Epoch [11/20], Step [150/164], Loss: 0.3633\n",
      "Start validation #11\n",
      "Validation #11  Average Loss: 0.4628, mIoU: 0.2696\n",
      "Best performance at epoch: 11\n",
      "Save model in ./saved\n",
      "Epoch [12/20], Step [25/164], Loss: 0.3132\n",
      "Epoch [12/20], Step [50/164], Loss: 0.2283\n",
      "Epoch [12/20], Step [75/164], Loss: 0.3237\n",
      "Epoch [12/20], Step [100/164], Loss: 0.6543\n",
      "Epoch [12/20], Step [125/164], Loss: 0.3567\n",
      "Epoch [12/20], Step [150/164], Loss: 0.2948\n",
      "Start validation #12\n",
      "Validation #12  Average Loss: 0.4873, mIoU: 0.2633\n",
      "Epoch [13/20], Step [25/164], Loss: 0.2150\n",
      "Epoch [13/20], Step [50/164], Loss: 0.2707\n",
      "Epoch [13/20], Step [75/164], Loss: 0.3245\n",
      "Epoch [13/20], Step [100/164], Loss: 0.4786\n",
      "Epoch [13/20], Step [125/164], Loss: 0.3047\n",
      "Epoch [13/20], Step [150/164], Loss: 0.2316\n",
      "Start validation #13\n",
      "Validation #13  Average Loss: 0.4627, mIoU: 0.2705\n",
      "Best performance at epoch: 13\n",
      "Save model in ./saved\n",
      "Epoch [14/20], Step [25/164], Loss: 0.2577\n",
      "Epoch [14/20], Step [50/164], Loss: 0.2627\n",
      "Epoch [14/20], Step [75/164], Loss: 0.2230\n",
      "Epoch [14/20], Step [100/164], Loss: 0.2074\n",
      "Epoch [14/20], Step [125/164], Loss: 0.3834\n",
      "Epoch [14/20], Step [150/164], Loss: 0.2464\n",
      "Start validation #14\n",
      "Validation #14  Average Loss: 0.4978, mIoU: 0.2837\n",
      "Epoch [15/20], Step [25/164], Loss: 0.2241\n",
      "Epoch [15/20], Step [50/164], Loss: 0.2115\n",
      "Epoch [15/20], Step [75/164], Loss: 0.2797\n",
      "Epoch [15/20], Step [100/164], Loss: 0.2670\n",
      "Epoch [15/20], Step [125/164], Loss: 0.2507\n",
      "Epoch [15/20], Step [150/164], Loss: 0.2656\n",
      "Start validation #15\n",
      "Validation #15  Average Loss: 0.4736, mIoU: 0.2773\n",
      "Epoch [16/20], Step [25/164], Loss: 0.1381\n",
      "Epoch [16/20], Step [50/164], Loss: 0.1837\n",
      "Epoch [16/20], Step [75/164], Loss: 0.2243\n",
      "Epoch [16/20], Step [100/164], Loss: 0.2252\n",
      "Epoch [16/20], Step [125/164], Loss: 0.2023\n",
      "Epoch [16/20], Step [150/164], Loss: 0.2156\n",
      "Start validation #16\n",
      "Validation #16  Average Loss: 0.5090, mIoU: 0.2770\n",
      "Epoch [17/20], Step [25/164], Loss: 0.1505\n",
      "Epoch [17/20], Step [50/164], Loss: 0.1672\n",
      "Epoch [17/20], Step [75/164], Loss: 0.1491\n",
      "Epoch [17/20], Step [100/164], Loss: 0.1689\n",
      "Epoch [17/20], Step [125/164], Loss: 0.2030\n",
      "Epoch [17/20], Step [150/164], Loss: 0.2299\n",
      "Start validation #17\n",
      "Validation #17  Average Loss: 0.5484, mIoU: 0.2707\n",
      "Epoch [18/20], Step [25/164], Loss: 0.2821\n",
      "Epoch [18/20], Step [50/164], Loss: 0.0879\n",
      "Epoch [18/20], Step [75/164], Loss: 0.2332\n",
      "Epoch [18/20], Step [100/164], Loss: 0.1633\n",
      "Epoch [18/20], Step [125/164], Loss: 0.1805\n",
      "Epoch [18/20], Step [150/164], Loss: 0.2135\n",
      "Start validation #18\n",
      "Validation #18  Average Loss: 0.5619, mIoU: 0.2827\n",
      "Epoch [19/20], Step [25/164], Loss: 0.1478\n",
      "Epoch [19/20], Step [50/164], Loss: 0.2028\n",
      "Epoch [19/20], Step [75/164], Loss: 0.1534\n",
      "Epoch [19/20], Step [100/164], Loss: 0.2100\n",
      "Epoch [19/20], Step [125/164], Loss: 0.1550\n",
      "Epoch [19/20], Step [150/164], Loss: 0.1805\n",
      "Start validation #19\n",
      "Validation #19  Average Loss: 0.5503, mIoU: 0.2560\n",
      "Epoch [20/20], Step [25/164], Loss: 0.1994\n",
      "Epoch [20/20], Step [50/164], Loss: 0.0899\n",
      "Epoch [20/20], Step [75/164], Loss: 0.1627\n",
      "Epoch [20/20], Step [100/164], Loss: 0.1543\n",
      "Epoch [20/20], Step [125/164], Loss: 0.2439\n",
      "Epoch [20/20], Step [150/164], Loss: 0.1047\n",
      "Start validation #20\n",
      "Validation #20  Average Loss: 0.5804, mIoU: 0.2858\n"
     ]
    }
   ],
   "source": [
    "train(CFG.num_epochs, model, train_loader, val_loader, criterion, optimizer, saved_dir, val_every, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 저장된 model 불러오기 (학습된 이후) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T19:44:21.050200Z",
     "start_time": "2021-04-16T19:44:20.802200Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# best model 저장된 경로\n",
    "model_path = f'./saved/{CFG.version}.pt'\n",
    "\n",
    "# best model 불러오기\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "# 추론을 실행하기 전에는 반드시 설정 (batch normalization, dropout 를 평가 모드로 설정)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T19:44:24.939227Z",
     "start_time": "2021-04-16T19:44:24.518228Z"
    }
   },
   "outputs": [],
   "source": [
    "# 첫번째 batch의 추론 결과 확인\n",
    "for imgs, image_infos in test_loader:\n",
    "    image_infos = image_infos\n",
    "    temp_images = imgs\n",
    "    \n",
    "    model.eval()\n",
    "    # inference\n",
    "    outs = model(torch.stack(temp_images).to(device))\n",
    "    oms = torch.argmax(outs.squeeze(), dim=1).detach().cpu().numpy()\n",
    "    \n",
    "    break\n",
    "\n",
    "i = 3\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(16, 16))\n",
    "\n",
    "print('Shape of Original Image :', list(temp_images[i].shape))\n",
    "print('Shape of Predicted : ', list(oms[i].shape))\n",
    "print('Unique values, category of transformed mask : \\n', [{int(i),category_names[int(i)]} for i in list(np.unique(oms[i]))])\n",
    "\n",
    "# Original image\n",
    "ax1.imshow(temp_images[i].permute([1,2,0]))\n",
    "ax1.grid(False)\n",
    "ax1.set_title(\"Original image : {}\".format(image_infos[i]['file_name']), fontsize = 15)\n",
    "\n",
    "# Predicted\n",
    "ax2.imshow(oms[i])\n",
    "ax2.grid(False)\n",
    "ax2.set_title(\"Predicted : {}\".format(image_infos[i]['file_name']), fontsize = 15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## submission을 위한 test 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T19:44:27.469285Z",
     "start_time": "2021-04-16T19:44:27.456021Z"
    }
   },
   "outputs": [],
   "source": [
    "def test(model, data_loader, device):\n",
    "    size = 256\n",
    "    transform = A.Compose([A.Resize(256, 256)])\n",
    "    print('Start prediction.')\n",
    "    model.eval()\n",
    "    \n",
    "    file_name_list = []\n",
    "    preds_array = np.empty((0, size*size), dtype=np.long)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step, (imgs, image_infos) in enumerate(test_loader):\n",
    "\n",
    "            # inference (512 x 512)\n",
    "            outs = model(torch.stack(imgs).to(device))\n",
    "            oms = torch.argmax(outs.squeeze(), dim=1).detach().cpu().numpy()\n",
    "            \n",
    "            # resize (256 x 256)\n",
    "            temp_mask = []\n",
    "            for img, mask in zip(np.stack(temp_images), oms):\n",
    "                transformed = transform(image=img, mask=mask)\n",
    "                mask = transformed['mask']\n",
    "                temp_mask.append(mask)\n",
    "\n",
    "            oms = np.array(temp_mask)\n",
    "            \n",
    "            oms = oms.reshape([oms.shape[0], size*size]).astype(int)\n",
    "            preds_array = np.vstack((preds_array, oms))\n",
    "            \n",
    "            file_name_list.append([i['file_name'] for i in image_infos])\n",
    "    print(\"End prediction.\")\n",
    "    file_names = [y for x in file_name_list for y in x]\n",
    "    \n",
    "    return file_names, preds_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## submission.csv 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T19:45:42.235310Z",
     "start_time": "2021-04-16T19:44:30.499016Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sample_submisson.csv 열기\n",
    "submission = pd.read_csv('./submission/sample_submission.csv', index_col=None)\n",
    "\n",
    "# test set에 대한 prediction\n",
    "file_names, preds = test(model, test_loader, device)\n",
    "\n",
    "# PredictionString 대입\n",
    "for file_name, string in zip(file_names, preds):\n",
    "    submission = submission.append({\"image_id\" : file_name, \"PredictionString\" : ' '.join(str(e) for e in string.tolist())}, \n",
    "                                   ignore_index=True)\n",
    "\n",
    "# submission.csv로 저장\n",
    "submission.to_csv(f\"./submission/{CFG.version}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyper-param 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "297.278px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
